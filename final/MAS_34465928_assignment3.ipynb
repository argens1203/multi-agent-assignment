{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Multi Agent Deep Q-Learning**\n",
    "\n",
    "## **Problem Description**\n",
    "\n",
    "This task involves 4 agents of 2 types navigating a 5x5 grid world to reach a goal position B. An agent must meet an agent of opposite type to exchange secret before they can end the episode by reaching B. Agents cannot exchange secret at goal position B and only one agent is needed to reach B to terminate the episode.\n",
    "\n",
    "All positions are randomised and can overlap.\n",
    "\n",
    "## **Chosen contract options**\n",
    "The following options are chosen\n",
    "- Nearest opposite type agent (location)\n",
    "- Off the job training (not used)\n",
    "- Clock\n",
    "\n",
    "## **Methodology**\n",
    "\n",
    "The agent uses Deep Q-learning to learn an optimal policy. The parameters for the agent are as follow\n",
    "\n",
    "|    **Parameter**    | **Value** |\n",
    "|:---------------|-----------:|\n",
    "| gamma (reward discount) | 0.9 |\n",
    "| optimizer.alpha | 0.001 |\n",
    "| dqn.hidden_size | [250, 250] |\n",
    "| epsilon_max | 1.0 |\n",
    "| epsilon_min | 0.005 |\n",
    "| eps_decay_final_step | 19900 |\n",
    "| replay buffer size | 100 |\n",
    "| batch size | 32 |\n",
    "| network copy frequency | 80 |\n",
    "\n",
    "The agent chooses an action with `ε`-greedy policy then stores the experience in replay memory D. When there is enough experience in D, randomly extract a minibatch of experiences and calculate the target value by\n",
    "$$\n",
    "\\begin{cases}\n",
    "    y_i = r_i & \\text{if done} \\\\\n",
    "    y_i = r_i + \\gamma \\max_{a'} \\^{Q}(s'_i, a'_i) & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then do one step of learning.\n",
    "\n",
    "To facilitate convergence, three changes were made besides using the given API and implementing the learning algorithm.\n",
    "\n",
    "1. **torch.tensors** are used throughout and GPU/MPS acceleration used whenever available\n",
    "2. maximum of 50 steps can be spent in one episode before resetting \n",
    "\n",
    "### **State Space**\n",
    "\n",
    "The state space is defined by the agent’s position on the grid, the closest location of agent of opposite type (A), and the location of goal (B). Also 1 extra bit is used for whether or not the agent has received the secret. Since our grid is 5 by 5, and we need to use one-hot encoding for machine learning, this translates into 76 bits, 25 for each position / location.\n",
    "\n",
    "### **Action Space**\n",
    "\n",
    "The agent can perform one of four actions at any given time:\n",
    "\n",
    "- **Move North**\n",
    "- **Move South**\n",
    "- **Move West**\n",
    "- **Move East**\n",
    "- **Stay Still**\n",
    "\n",
    "These actions move the agent one step in the corresponding direction unless the movement would result in the agent hitting a wall, in which case the agent remains in the same position.\n",
    "\n",
    "### **Reward Structure**\n",
    "\n",
    "The reward structure is designed to guide the agent toward efficiently solving the task:\n",
    "\n",
    "|    **Event**    | **Reward** |\n",
    "|:---------------|-----------:|\n",
    "| Receiving Secret (first time) | +20 |\n",
    "| Reaching Goal with Secret | +50 |\n",
    "| Every Round | -1 |\n",
    "| Hitting a wall | -10 |\n",
    "\n",
    "Multiple rewards may accumulate at each round. For example, if a type 1 agent, not having the secret, moves into a cell occupied by a type 2 agent in a non-goal cell results in a reward of 19. This reward system incentivizes the agent to exchange secret and reach the goal while penalizing unnecessary movements and collisions with walls.\n",
    "\n",
    "If the agent reaches the goal position without the secret, the reward would be -1 and the game would not terminate.\n",
    "\n",
    "## **Performance Metrics**\n",
    "\n",
    "### **ML Loss**\n",
    "To show the progress of learning wtih Deep Q-Learning, there is a graph showing average MSE loss in each episode. This is the returned value from given skeleton divided by batch_size, then averaged over all learnings, to show the loss per episode.\n",
    "\n",
    "### **Excess Step**\n",
    "\n",
    "To visualize the agents' performance, the excess step is calculated by the number of steps taken minus the minimum steps possible in a scenario. In both calculations, we calculate with **Clock** enabled.\n",
    "\n",
    "During training and testing, we reorder the agents' move order to make sure the agents move according to descending order to the goal position to make sure the minimum steps possible is actually achieveable.\n",
    "\n",
    "\n",
    "#### **Minimum Step Calculation**\n",
    "\n",
    "To calculate the minimum number of steps needed, we loop between all possible pairs of agents and solve the subproblem when there were only 2 agents on the grid. We find the minimum steps for each scenario with 2 agents, and the minimum of all scenarios is the number we wanted.\n",
    "\n",
    "To solve the sub problem, we first observe that the answer must be closely correlated to the max(distance1, distance_2), where distance_1 and distance_2 are the corresponding manhattan distances of the agents and the goal position. This is because by meeting up and going to the goal position, the total distance travelled is simply the larger agent-goal distance. It does not matter whether the other agent decides to wait near goal position or move towards the further agent.\n",
    "\n",
    "Hence we first calculate the manhattan distances.\n",
    "\n",
    "If they can meet up without going through the goal position, no extra step is added, otherwise 2 is added for the closer agent to go through the goal and meet at wherever convenient.\n",
    "\n",
    "Then the larger distance can be reduced by 1 if clock is enabled. And the result is the max of both modified distances.\n",
    "```\n",
    "def calculate_min_step_for_two(self, one_pos, two_pos, goal_pos, clock=False):\n",
    "    one_dist = self.calc_mht_dist(one_pos, goal_pos)\n",
    "    two_dist = self.calc_mht_dist(two_pos, goal_pos)\n",
    "\n",
    "    smaller, larger = min(one_dist, two_dist), max(one_dist, two_dist)\n",
    "    smaller = smaller + (\n",
    "        0\n",
    "        if self.line_passing_through_goal_can_cut(one_pos, two_pos, goal_pos)\n",
    "        else 2\n",
    "    )\n",
    "    smaller, larger = min(smaller, larger), max(smaller, larger)\n",
    "\n",
    "    return max(smaller, larger - (1 if clock else 0))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants.py\n",
    "\n",
    "import torch\n",
    "\n",
    "side = 5\n",
    "action_size = 5\n",
    "action_space = [(0, -1), (0, 1), (-1, 0), (1, 0), (0, 0)]\n",
    "\n",
    "state_size = 5 * 5 * 3 + 1\n",
    "device = torch.device(\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "dtype = torch.float32\n",
    "# if device.type == \"mps\" else torch.float64\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py\n",
    "\n",
    "from typing import TYPE_CHECKING, Tuple, List\n",
    "from abc import abstractmethod, ABC\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    pass\n",
    "\n",
    "\n",
    "class ExpBuffer:\n",
    "    def __init__(self, max):\n",
    "        self.max = max\n",
    "        self.itr = 0\n",
    "        self.has_reached = False\n",
    "\n",
    "        self.states = torch.empty((self.max, state_size), dtype=dtype)\n",
    "        self.actions = torch.empty((self.max,), dtype=dtype)\n",
    "        self.rewards = torch.empty((self.max,), dtype=dtype)\n",
    "        self.next_states = torch.empty((self.max, state_size), dtype=dtype)\n",
    "        self.is_terminals = torch.empty((self.max,), dtype=torch.bool)\n",
    "        pass\n",
    "\n",
    "    def insert(self, state, action, reward, next_state, is_terminal):\n",
    "        self.itr %= self.max\n",
    "        self.states[self.itr] = state\n",
    "        self.actions[self.itr] = action\n",
    "        self.rewards[self.itr] = reward\n",
    "        self.next_states[self.itr] = next_state\n",
    "        self.is_terminals[self.itr] = is_terminal\n",
    "        self.itr += 1\n",
    "\n",
    "        if self.itr >= self.max:\n",
    "            self.has_reached = True\n",
    "\n",
    "    def extract(self, batch_size) -> Tuple[List[List[int]], List[int], List[float]]:\n",
    "        indices = np.random.randint(\n",
    "            0, self.max if self.has_reached else self.itr, batch_size\n",
    "        )\n",
    "        return (\n",
    "            self.states[indices],\n",
    "            self.actions[indices],\n",
    "            self.rewards[indices],\n",
    "            self.next_states[indices],\n",
    "            self.is_terminals[indices],\n",
    "        )\n",
    "\n",
    "    def clear(self):\n",
    "        self.__init__(self.max)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "\n",
    "class Agent(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dqn,\n",
    "        buffer,\n",
    "        gamma=0.997,\n",
    "        batch_size=200,\n",
    "    ):\n",
    "        self.dqn = dqn\n",
    "        self.buffer = buffer\n",
    "        self.actions = action_space\n",
    "\n",
    "        # Agent Properties\n",
    "        self.total_reward = 0\n",
    "        self.have_secret = False\n",
    "\n",
    "        # Initialize Q Table for all state-action to be 0\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Initialize Learning param\n",
    "        self.epsilon = 1  # Epsilon is updated at Grid level\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.learning = True\n",
    "\n",
    "    # ----- Core Functions ----- #\n",
    "    def choose_action(self, state: torch.tensor, choose_best: bool) -> Tuple[int, int]:\n",
    "        if (\n",
    "            not choose_best\n",
    "            and np.random.rand() < self.epsilon  # Epsilon is updated at Grid level\n",
    "        ):\n",
    "            return random.choice(self.actions)\n",
    "        else:\n",
    "            # Extract immutable state information\n",
    "            idx = torch.argmax(self.dqn.get_qvals(state))\n",
    "            return self.actions[idx]\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        state: torch.tensor,\n",
    "        action: Tuple[int, int],\n",
    "        reward: int,\n",
    "        next_state: torch.tensor,\n",
    "        is_terminal: bool,\n",
    "    ):\n",
    "        self.total_reward += reward\n",
    "        if not self.learning:\n",
    "            return None, None\n",
    "\n",
    "        self.buffer.insert(\n",
    "            state, self.actions.index(action), reward, next_state, is_terminal\n",
    "        )\n",
    "        loss = None\n",
    "        if len(self.buffer) >= self.batch_size:\n",
    "            states, actions, rewards, next_states, is_terminals = self.buffer.extract(\n",
    "                self.batch_size\n",
    "            )\n",
    "            rewards = rewards.to(device)\n",
    "            targets = self.gamma * self.dqn.get_maxQ(next_states) + rewards\n",
    "\n",
    "            # For terminal states, target_val is reward\n",
    "            indices = is_terminals.nonzero().to(device)\n",
    "            targets[indices] = rewards[indices]\n",
    "\n",
    "            loss = self.dqn.train_one_step(states, actions, targets)\n",
    "\n",
    "        return loss, self.epsilon\n",
    "\n",
    "    # ----- Public Functions ----- #\n",
    "    def have_secret_(self, new_value: bool):\n",
    "        self.have_secret = new_value\n",
    "\n",
    "    def reset(self):\n",
    "        self.have_secret = False\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def get_total_reward(self):\n",
    "        return self.total_reward\n",
    "\n",
    "    def enable_learning(self):\n",
    "        if not self.learning:\n",
    "            self.buffer.clear()\n",
    "        self.learning = True\n",
    "\n",
    "    def disable_learning(self):\n",
    "        self.learning = False\n",
    "\n",
    "    # ----- Private Functions ----- #\n",
    "    @abstractmethod\n",
    "    def get_type(self):\n",
    "        pass\n",
    "\n",
    "    def is_different_type(self, other: \"Agent\"):\n",
    "        return other.get_type() != self.get_type()\n",
    "\n",
    "\n",
    "class Agent1(Agent):\n",
    "    def get_type(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "class Agent2(Agent):\n",
    "    def get_type(self):\n",
    "        return 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given.py\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, state_size, action_size=4):\n",
    "        l1 = state_size\n",
    "        l2 = 250\n",
    "        l3 = 250\n",
    "        l5 = action_size\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(l1, l2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(l2, l3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(l3, l5),\n",
    "        ).to(device)\n",
    "\n",
    "        self.model2 = copy.deepcopy(self.model).to(device)\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.learning_rate = 1e-3\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=self.learning_rate\n",
    "        )\n",
    "\n",
    "    # The function \"update_target\" copies the state of the prediction network to the target network. You need to use this in regular intervals.\n",
    "    def update_target(self):\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    # The function \"get_qvals\" returns a numpy list of qvals for the state given by the argument based on the prediction network.\n",
    "    def get_qvals(self, state):\n",
    "        q_values = self.model(state.to(device)).to(device)\n",
    "        return q_values\n",
    "\n",
    "    # The function \"get_maxQ\" returns the maximum q-value for the state given by the argument based on the target network.\n",
    "    def get_maxQ(self, state):\n",
    "        return torch.max(self.model2(state.to(device)), dim=1).values.float()\n",
    "\n",
    "    # The function \"train_one_step_new\" performs a single training step.\n",
    "    # It returns the current loss (only needed for debugging purposes).\n",
    "    # Its parameters are three parallel lists: a minibatch of states, a minibatch of actions,\n",
    "    # a minibatch of the corresponding TD targets and the discount factor.\n",
    "    def train_one_step(self, states, actions, targets):\n",
    "        # state1_batch = torch.cat([torch.from_numpy(s).float() for s in states])\n",
    "        state1_batch = states.to(device)\n",
    "        action_batch = actions.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # print(action_batch.shape)\n",
    "        # print(state1_batch.shape)\n",
    "        Q1 = self.model(state1_batch)\n",
    "        X = Q1.gather(dim=1, index=action_batch.long().unsqueeze(dim=1)).squeeze()\n",
    "        Y = targets.clone().detach().to(device).float()\n",
    "        loss = self.loss_fn(X, Y)\n",
    "        # print(loss)\n",
    "        self.optimizer.zero_grad()\n",
    "        # torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5000)\n",
    "        loss.backward()\n",
    "\n",
    "        # total_norm = 0\n",
    "        # for p in self.model.parameters():\n",
    "        #     param_norm = p.grad.data.norm(2)\n",
    "        #     total_norm += param_norm.item() ** 2\n",
    "        # total_norm = total_norm ** (1.0 / 2)\n",
    "        # print(total_norm)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        return loss.item() / len(X)\n",
    "\n",
    "    def save(self, prefix):\n",
    "        torch.save(self.model.state_dict(), f\"{prefix}_1.pth\")\n",
    "        torch.save(self.model2.state_dict(), f\"{prefix}_2.pth\")\n",
    "\n",
    "    def load(self, prefix):\n",
    "        try:\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(f\"{prefix}_1.pth\", weights_only=False, map_location=device)\n",
    "            )\n",
    "            self.model2.load_state_dict(\n",
    "                torch.load(f\"{prefix}_2.pth\", weights_only=False, map_location=device)\n",
    "            )\n",
    "        except:\n",
    "            print(\"load state_dict failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage.py\n",
    "\n",
    "from multiprocessing import Array\n",
    "from typing import List\n",
    "\n",
    "class Storage:\n",
    "    def __init__(self, max_itr: int):\n",
    "        self.itr = 0\n",
    "        self.max_itr = max_itr\n",
    "\n",
    "        # TODO: max_itr could be dynamic\n",
    "        self.iterations = Array(\"i\", range(max_itr))\n",
    "        self.excess_step = Array(\"i\", max_itr)\n",
    "        self.epsilon = Array(\"f\", max_itr)\n",
    "        self.test_loss = []\n",
    "        self.ml_losses = []\n",
    "\n",
    "        self.step_count_hist = Array(\"i\", 51)\n",
    "        self.excess_step_hist = Array(\"i\", 51)\n",
    "\n",
    "    def reset_counter(self):\n",
    "        self.itr = 0\n",
    "\n",
    "    def append_excess_epsilon(self, excess_step: int, epsilon: float):\n",
    "        if self.itr >= self.max_itr:\n",
    "            self.itr = 0\n",
    "        self.excess_step[self.itr] = excess_step\n",
    "        self.epsilon[self.itr] = epsilon\n",
    "        self.itr += 1\n",
    "\n",
    "    def append_step_count_hist(self, step_count: int):\n",
    "        self.step_count_hist[step_count] += 1\n",
    "\n",
    "    def append_excess_step_hist(self, excess_step: int):\n",
    "        self.excess_step_hist[excess_step] += 1\n",
    "\n",
    "    def append_test_loss(self, test_loss: int):\n",
    "        self.test_loss.append(test_loss)\n",
    "\n",
    "    def reset_test_loss(self):\n",
    "        self.test_loss = []\n",
    "\n",
    "    def append_ml_losses(self, ml_losses: float):\n",
    "        self.ml_losses.append(ml_losses)\n",
    "\n",
    "    def get_all(self):\n",
    "        return self.iterations, self.losses, self.epsilon, self.test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view.py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from matplotlib.widgets import Button\n",
    "from typing import Tuple, TypeAlias, TYPE_CHECKING, List\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "Coordinates: TypeAlias = Tuple[float, float, float, float]\n",
    "\n",
    "\n",
    "class IVisual(ABC):\n",
    "\n",
    "    # Getting Info\n",
    "    @abstractmethod\n",
    "    def get_agent_info(self) -> List[Tuple[Tuple[int, int], bool]]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_total_reward(self) -> int:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_min_step(self) -> int:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_size(self) -> Tuple[int, int]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_goal_positions(self) -> List[Tuple[int, int]]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_agent_positions(self) -> List[Tuple[int, int]]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def has_ended(self) -> bool:\n",
    "        pass\n",
    "\n",
    "    # Functions\n",
    "    @abstractmethod\n",
    "    def toggle_auto_reset(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def next(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    # Trainings/Testing\n",
    "    @abstractmethod\n",
    "    def train(self, itr=1):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def test(self, itr=1):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Visualization:\n",
    "    def __init__(self, storage: \"Storage\", grid: \"Grid\"):\n",
    "        self.storage = storage\n",
    "        self.grid = grid\n",
    "        self.fig, self.ax = plt.subplots()\n",
    "        self.animating = False\n",
    "\n",
    "    def show(self):\n",
    "        assert self.grid is not None\n",
    "\n",
    "        self.add_ui_elements()\n",
    "        self.fig.canvas.mpl_connect(\"close_event\", self.on_close)\n",
    "        self.ani = animation.FuncAnimation(\n",
    "            self.fig, self.draw, frames=self.frames, interval=200, save_count=100\n",
    "        )\n",
    "        self.animating = True\n",
    "        plt.show()\n",
    "\n",
    "    def get_info(self):\n",
    "        agent_info = self.grid.get_agent_info()\n",
    "        step_count = self.grid.get_step_count()\n",
    "        min_step = self.grid.get_min_step()\n",
    "        return agent_info, step_count, min_step\n",
    "\n",
    "    def frames(self):\n",
    "        while True:\n",
    "            if self.animating:\n",
    "                self.grid.next()\n",
    "                yield self.get_info()\n",
    "            else:\n",
    "                yield self.get_info()\n",
    "\n",
    "    # ----- ----- ----- ----- Drawing Functions  ----- ----- ----- ----- #\n",
    "\n",
    "    def draw(self, args):\n",
    "        agent_info, step_count, min_step = args\n",
    "\n",
    "        self.ax.clear()\n",
    "        self.draw_grid()\n",
    "        self.draw_agent(agent_info)\n",
    "\n",
    "        self.step_count.set_text(f\"Step: {step_count}\")\n",
    "        self.min_step.set_text(f\"Min Step: {min_step}\")\n",
    "\n",
    "        # Check if the environment is terminal\n",
    "        if self.grid.has_ended():\n",
    "            self.draw_complete()\n",
    "\n",
    "        # Early return if animating, since animation automatically refreshes canvas\n",
    "        if self.animating:\n",
    "            return\n",
    "\n",
    "        self.fig.canvas.draw()\n",
    "\n",
    "    def draw_grid(self):\n",
    "        width, height = self.grid.get_size()\n",
    "        for x in range(width):\n",
    "            for y in range(height):\n",
    "                rect = patches.Rectangle(\n",
    "                    (x, y), 1, 1, linewidth=1, edgecolor=\"black\", facecolor=\"white\"\n",
    "                )\n",
    "                self.ax.add_patch(rect)\n",
    "        self.ax.set_xlim(0, width)\n",
    "        self.ax.set_ylim(height, 0)\n",
    "        self.ax.set_aspect(\"equal\")\n",
    "\n",
    "        # Move x-axis labels to the top\n",
    "        self.ax.xaxis.set_label_position(\"top\")\n",
    "        self.ax.xaxis.tick_top()\n",
    "\n",
    "        # Draw target\n",
    "        tx, ty = self.grid.get_goal_positions()\n",
    "        target_patch = patches.Rectangle(\n",
    "            (tx, ty), 1, 1, linewidth=1, edgecolor=\"black\", facecolor=\"green\"\n",
    "        )\n",
    "        self.ax.add_patch(target_patch)\n",
    "\n",
    "    def draw_agent(self, info):\n",
    "        # Draw agent\n",
    "        for idx, (pos, type, has_item, step_count) in enumerate(info):\n",
    "            dx = [0, 0.5, 0, 0.5][idx]\n",
    "            dy = [0, 0, 0.5, 0.5][idx]\n",
    "            ax, ay = pos\n",
    "\n",
    "            if type == 1:\n",
    "                agent_color = \"red\" if has_item else \"pink\"\n",
    "            if type == 2:\n",
    "                agent_color = \"blue\" if has_item else \"cyan\"\n",
    "\n",
    "            agent_patch = patches.Rectangle(\n",
    "                (ax + dx, ay + dy),\n",
    "                0.5,\n",
    "                0.5,\n",
    "                linewidth=1,\n",
    "                edgecolor=\"black\",\n",
    "                facecolor=agent_color,\n",
    "            )\n",
    "            self.ax.add_patch(agent_patch)\n",
    "            self.ax.text(\n",
    "                ax + dx + 0.17,\n",
    "                ay + dy + 0.33,\n",
    "                step_count,\n",
    "                c=\"yellow\",\n",
    "                ma=\"center\",\n",
    "                size=\"large\",\n",
    "                weight=\"bold\",\n",
    "                # backgroundcolor=\"white\",\n",
    "            )\n",
    "\n",
    "    def draw_complete(self):\n",
    "        self.ax.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"Complete\",\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            transform=self.ax.transAxes,\n",
    "            fontsize=20,\n",
    "            color=\"red\",\n",
    "        )\n",
    "\n",
    "    # ----- ----- ----- ----- Render UI Element  ----- ----- ----- ----- #\n",
    "\n",
    "    def add_ui_elements(self):\n",
    "        self.init_buttons()\n",
    "        self.init_text()\n",
    "\n",
    "    def init_buttons(self):\n",
    "        self.toggle_anim_btn = None\n",
    "        self.toggle_auto_reset_btn = None\n",
    "\n",
    "        btn_template = [\n",
    "            (\"Next Step\", self.on_next),\n",
    "            (\"Reset\", self.on_reset),\n",
    "            (\"Anim\\nOn\", self.on_toggle_anim, \"toggle_anim_btn\"),\n",
    "            (\"Auto Reset\\nOn\", self.on_auto_reset, \"toggle_auto_reset_btn\"),\n",
    "        ]\n",
    "        self.buttons = []\n",
    "        x, y, w, h = 0.85, 0.01, 0.12, 0.075\n",
    "        for template in btn_template:\n",
    "            ref = None\n",
    "            try:\n",
    "                label, cb, ref = template\n",
    "            except:\n",
    "                label, cb = template\n",
    "            self.buttons.append(self.add_button([x, y, w, h], label, cb))\n",
    "            y += 0.1\n",
    "            if ref:\n",
    "                setattr(self, ref, self.buttons[-1])\n",
    "\n",
    "    def init_text(self):\n",
    "        # Add text box for cumulative reward\n",
    "        self.step_count = self.add_text([0.01, 0.01, 0.2, 0.075], f\"Step: 0\")\n",
    "\n",
    "        # Add text box for max reward\n",
    "        self.min_step = self.add_text(\n",
    "            [0.25, 0.01, 0.2, 0.075],\n",
    "            f\"Min Step: {self.grid.get_min_step()}\",\n",
    "        )\n",
    "\n",
    "    def add_button(self, coordinates: Coordinates, text, on_click):\n",
    "        axis = plt.axes(coordinates)\n",
    "        button = Button(axis, text)\n",
    "        button.on_clicked(on_click)\n",
    "\n",
    "        return button\n",
    "\n",
    "    def add_text(self, coordinates: Coordinates, text):\n",
    "        axis = plt.axes(coordinates)\n",
    "        textbox = axis.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            text,\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            transform=axis.transAxes,\n",
    "            fontsize=12,\n",
    "        )\n",
    "        axis.axis(\"off\")\n",
    "        return textbox\n",
    "\n",
    "    # ----- ----- ----- ----- Event Handlers  ----- ----- ----- ----- #\n",
    "\n",
    "    def on_close(self, e):\n",
    "        self.ani.event_source.stop()\n",
    "\n",
    "    def on_auto_reset(self, event):\n",
    "        is_on = self.grid.toggle_auto_reset()\n",
    "        if is_on:\n",
    "            self.toggle_auto_reset_btn.label.set_text(\"Auto Reset\\nOn\")\n",
    "        else:\n",
    "            self.toggle_auto_reset_btn.label.set_text(\"Auto Reset\\nOff\")\n",
    "        plt.show()\n",
    "\n",
    "    def on_toggle_anim(self, event):\n",
    "        if self.animating:\n",
    "            self.toggle_anim_btn.label.set_text(\"Anim\\nOff\")\n",
    "        else:\n",
    "            self.toggle_anim_btn.label.set_text(\"Anim\\nOn\")\n",
    "\n",
    "        self.animating = not self.animating\n",
    "        plt.show()\n",
    "\n",
    "    def on_reset(self, event):\n",
    "        self.grid.reset()\n",
    "        self.draw(self.get_info())\n",
    "\n",
    "    def on_next(self, e):\n",
    "        self.grid.next()\n",
    "        self.draw(self.get_info())\n",
    "\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self, storage: \"Storage\", keys=[]):\n",
    "        self.storage = storage\n",
    "        self.keys = keys\n",
    "\n",
    "        self.fig, self.axs = plt.subplots(1, len(self.keys))\n",
    "        if len(self.keys) == 1:\n",
    "            self.axs = [self.axs]\n",
    "        self.ani = animation.FuncAnimation(\n",
    "            self.fig, self.draw, frames=self.frames, interval=100, save_count=100\n",
    "        )\n",
    "\n",
    "    def frames(self):\n",
    "        while True:\n",
    "            yield None\n",
    "\n",
    "    def show(self):\n",
    "        plt.show()\n",
    "\n",
    "    # Compulsory unused argument\n",
    "    def draw(self, args):\n",
    "        for i, k in enumerate(self.keys):\n",
    "            self.plot(\n",
    "                self.axs[i],\n",
    "                range(len(getattr(self.storage, k))),  # self.storage.iterations\n",
    "                getattr(self.storage, k),\n",
    "                k,\n",
    "            )\n",
    "\n",
    "    def plot(self, ax, itr, values, key):\n",
    "        ax.plot(itr, values, color=\"blue\", label=\"Loss\")\n",
    "        ax.set_title(key.title())\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.set_ylabel(key.title())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid.py\n",
    "\n",
    "import torch\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import TYPE_CHECKING, List, Tuple, Dict, Set\n",
    "\n",
    "import random\n",
    "\n",
    "import datetime\n",
    "\n",
    "class Controller:\n",
    "    # Iterate by number of games\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.auto_reset = True\n",
    "\n",
    "    def toggle_auto_reset(self):\n",
    "        self.auto_reset = not self.auto_reset\n",
    "        return self.auto_reset\n",
    "\n",
    "    def next(self):\n",
    "        if self.has_ended() and self.auto_reset:\n",
    "            self.reset()\n",
    "        else:\n",
    "            self.step(is_testing=True)\n",
    "        return\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, storage, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.storage = storage\n",
    "        self.learners = [1, 2]\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        itr=1,\n",
    "        upd_freq=100,\n",
    "        eps_min=5e-2,\n",
    "        eps_decay_final_step=15e3,\n",
    "        max_grad_norm=5e3,\n",
    "        dqn1=None,\n",
    "        dqn2=None,\n",
    "    ):\n",
    "        def calc_eps(start_eps, end_eps, step, final_step):\n",
    "            return (\n",
    "                start_eps + (end_eps - start_eps) * min(step, final_step) / final_step\n",
    "            )\n",
    "\n",
    "        start = datetime.datetime.now()\n",
    "        print(f\"Start Time: {start}\")\n",
    "\n",
    "        self.enable_learning(agent_type=1)\n",
    "        self.enable_learning(agent_type=2)\n",
    "\n",
    "        for i in range(itr):\n",
    "            # Copy Q to Q_hat\n",
    "            if i % upd_freq == 0:\n",
    "                for dqn in [dqn1, dqn2]:\n",
    "                    dqn.update_target()\n",
    "\n",
    "            # Decay epsilon linearly, reaching eps_min at eps_decay_final_step\n",
    "            epsilon = calc_eps(1, eps_min, i, eps_decay_final_step)\n",
    "            for agent in self.agents:\n",
    "                agent.epsilon = epsilon\n",
    "\n",
    "            self.reset()\n",
    "            (excess_step, reward, epsilon, ml_losses, step_count) = self.play_one_game(\n",
    "                is_testing=False\n",
    "            )\n",
    "\n",
    "            self.storage.append_ml_losses(sum(ml_losses) / len(ml_losses))\n",
    "            self.storage.append_excess_epsilon(excess_step, epsilon)\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(\n",
    "                    f\"Epoch: {i+1}/{itr} -- Time Elapsed: {datetime.datetime.now() - start}\"\n",
    "                )\n",
    "        return self.storage.ml_losses\n",
    "\n",
    "    def small_test(self, itr=1):\n",
    "        total_loss = 0\n",
    "        total_step_count = 0\n",
    "        success_count = 0\n",
    "\n",
    "        self.disable_learning(agent_type=1)\n",
    "        self.disable_learning(agent_type=2)\n",
    "\n",
    "        for i in range(itr):\n",
    "            self.reset()\n",
    "            excess_step, reward, eps, ml_losses, step_count = self.play_one_game(\n",
    "                is_testing=True\n",
    "            )\n",
    "            self.storage.append_excess_step_hist(excess_step)\n",
    "            if step_count < 15:\n",
    "                success_count += 1\n",
    "            total_loss += excess_step\n",
    "            total_step_count += step_count\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(\n",
    "                    f\"Ep {i + 1}: Avg = {total_step_count / (i + 1)}; Excess = {total_loss / (i + 1)}; Success = {success_count/(i + 1)}\"\n",
    "                )\n",
    "        return total_loss / itr\n",
    "\n",
    "    def test(self, max_itr=None):\n",
    "        itr = 0\n",
    "\n",
    "        self.disable_learning(agent_type=1)\n",
    "        self.disable_learning(agent_type=2)\n",
    "\n",
    "        possible_loc = [(x, y) for x in range(side) for y in range(side)]\n",
    "        total_step_count = 0\n",
    "        excess_step_count = 0\n",
    "        success_count = 0\n",
    "        count = 0\n",
    "\n",
    "        for goal_pos in possible_loc:\n",
    "            for p1 in possible_loc:\n",
    "                for p2 in possible_loc:\n",
    "                    for p3 in possible_loc:\n",
    "                        for p4 in possible_loc:\n",
    "                            if max_itr is not None and itr >= max_itr:\n",
    "                                return\n",
    "                            self.agent_positions = [p1, p2, p3, p4]\n",
    "                            self.goal_pos = goal_pos\n",
    "                            self.reset(random_pos=False)\n",
    "\n",
    "                            (excess_step, reward, epsilon, ml_loss, step_count) = (\n",
    "                                self.play_one_game(is_testing=True)\n",
    "                            )\n",
    "\n",
    "                            self.storage.append_step_count_hist(step_count)\n",
    "                            self.storage.append_excess_step_hist(excess_step)\n",
    "\n",
    "                            total_step_count += step_count\n",
    "                            excess_step_count += excess_step\n",
    "                            if step_count < 15:\n",
    "                                success_count += 1\n",
    "                            count += 1\n",
    "\n",
    "                            if count % 100 == 0:\n",
    "                                print(\n",
    "                                    f\"Ep {count}: Avg = {total_step_count / count}; Excess = {excess_step_count / count}; Success = {success_count / count}\"\n",
    "                                )\n",
    "                            itr += 1\n",
    "\n",
    "    def enable_learning(self, agent_type):\n",
    "        if agent_type not in self.learners:\n",
    "            self.learners.append(agent_type)\n",
    "        for a in [a for a in self.agents if a.get_type() == agent_type]:\n",
    "            a.enable_learning()\n",
    "\n",
    "    def disable_learning(self, agent_type):\n",
    "        if agent_type in self.learners:\n",
    "            self.learners.remove(agent_type)\n",
    "        for a in [a for a in self.agents if a.get_type() == agent_type]:\n",
    "            a.disable_learning()\n",
    "\n",
    "    def play_one_game(self, is_testing=False, **kwargs):\n",
    "        max_step_count = 50\n",
    "        self.step_count = 1\n",
    "        ml_losses = []\n",
    "        epsilons = []\n",
    "        while not self.has_ended() and self.step_count < max_step_count:\n",
    "            ml_loss, epsilon = self.step(is_testing)\n",
    "\n",
    "            if ml_loss is not None:\n",
    "                ml_losses.append(ml_loss)\n",
    "            if epsilon is not None:\n",
    "                epsilons.append(epsilon)\n",
    "        total_reward = sum(map(lambda a: a.get_total_reward(), self.agents))\n",
    "        loss = self.step_count - self.min_step\n",
    "        return (\n",
    "            loss,\n",
    "            total_reward,\n",
    "            None if len(epsilons) == 0 else epsilons[-1],\n",
    "            ml_losses,\n",
    "            self.step_count,\n",
    "        )\n",
    "\n",
    "\n",
    "class Visual:\n",
    "    def get_agent_info(self) -> List[Tuple[Tuple[int, int], int, bool]]:\n",
    "        \"\"\"\n",
    "        Output: List of\n",
    "                - Tuple of:\n",
    "                    - coordinate: (int, int)\n",
    "                    - type: int (1 or 2)\n",
    "                    - have_secret: bool\n",
    "        \"\"\"\n",
    "        agent_types = map(lambda agent: agent.get_type(), self.agents)\n",
    "        have_secrets = map(lambda agent: agent.have_secret, self.agents)\n",
    "        step_counts = [\n",
    "            self.step_count if idx < self.idx else self.step_count - 1\n",
    "            for idx in range(len(self.agents))\n",
    "        ]\n",
    "        return list(\n",
    "            zip(self.get_agent_positions(), agent_types, have_secrets, step_counts)\n",
    "        )\n",
    "\n",
    "    def get_total_reward(self):\n",
    "        return sum(map(lambda a: a.get_total_reward(), self.agents))\n",
    "\n",
    "    def get_min_step(self):\n",
    "        return self.min_step\n",
    "\n",
    "    def get_size(self):\n",
    "        return self.width, self.height\n",
    "\n",
    "    def get_goal_positions(self):\n",
    "        return self.goal_pos\n",
    "\n",
    "    def get_agent_positions(self):\n",
    "        return self.agent_positions\n",
    "\n",
    "    def has_ended(self) -> bool:\n",
    "        return self.goal_reached\n",
    "\n",
    "    def get_step_count(self) -> int:\n",
    "        # Incremented when last agent made a move\n",
    "        return self.step_count\n",
    "\n",
    "\n",
    "class GridUtil:\n",
    "    def calculate_min_step_for_two(self, one_pos, two_pos, goal_pos, clock=False):\n",
    "        one_dist = self.calc_mht_dist(one_pos, goal_pos)\n",
    "        two_dist = self.calc_mht_dist(two_pos, goal_pos)\n",
    "\n",
    "        smaller, larger = min(one_dist, two_dist), max(one_dist, two_dist)\n",
    "        smaller = smaller + (\n",
    "            0\n",
    "            if self.line_passing_through_goal_can_cut(one_pos, two_pos, goal_pos)\n",
    "            else 2\n",
    "        )\n",
    "        smaller, larger = min(smaller, larger), max(smaller, larger)\n",
    "\n",
    "        return max(smaller, larger - (1 if clock else 0))\n",
    "\n",
    "    def line_passing_through_goal_can_cut(self, one_pos, two_pos, goal_pos):\n",
    "        x1, y1 = one_pos\n",
    "        x2, y2 = two_pos\n",
    "        x, y = goal_pos\n",
    "        if x1 < x and x2 < x:\n",
    "            return True\n",
    "        if x1 > x and x2 > x:\n",
    "            return True\n",
    "        if y1 < y and y2 < y:\n",
    "            return True\n",
    "        if y1 > y and y2 > y:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def calc_mht_dist(self, pos1, pos2):\n",
    "        x1, y1 = pos1\n",
    "        x2, y2 = pos2\n",
    "        return abs(x1 - x2) + abs(y1 - y2)\n",
    "\n",
    "    def is_on_line_with_goal(self, pos, goal_pos):\n",
    "        x1, y1 = pos\n",
    "        x2, y2 = goal_pos\n",
    "        return x1 == x2 or y1 == y2\n",
    "\n",
    "    def calculate_min_step(self, clock=True):\n",
    "        ones = [idx for idx, agent in enumerate(self.agents) if agent.get_type() == 1]\n",
    "        twos = [idx for idx, agent in enumerate(self.agents) if agent.get_type() == 2]\n",
    "        ones_pos = [self.agent_positions[i] for i in ones]\n",
    "        twos_pos = [self.agent_positions[i] for i in twos]\n",
    "\n",
    "        min_step = 1e9\n",
    "        for one in ones_pos:\n",
    "            for two in twos_pos:\n",
    "                step = self.calculate_min_step_for_two(\n",
    "                    one, two, self.goal_pos, clock=clock\n",
    "                )\n",
    "                if step < min_step:\n",
    "                    min_step = step\n",
    "        return min_step\n",
    "\n",
    "    # Getting a random location in a grid, excluding certain locations\n",
    "    def get_random_pos(\n",
    "        self, width: int, height: int, exclude: List[Tuple[int, int]] = []\n",
    "    ) -> Tuple[int, int]:\n",
    "        while True:\n",
    "            position = (\n",
    "                random.randint(0, width - 1),\n",
    "                random.randint(0, height - 1),\n",
    "            )\n",
    "            if position not in exclude:\n",
    "                return position\n",
    "\n",
    "\n",
    "class Grid(Controller, Trainer, GridUtil, Visual, IVisual):\n",
    "    def __init__(\n",
    "        self,\n",
    "        width: int,\n",
    "        height: int,\n",
    "        agents: List[\"Agent\"],\n",
    "        storage: \"Storage\",\n",
    "    ):\n",
    "        super().__init__(storage=storage)\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.min_step = 0\n",
    "        self.step_count = 1\n",
    "\n",
    "        self.agents: List[\"Agent\"] = agents\n",
    "        self.idx: int = 0\n",
    "\n",
    "        self.set_interactive_tiles()\n",
    "        self.reorder_for_min_step()\n",
    "\n",
    "    def set_interactive_tiles(self):\n",
    "        # Assign goal to set position\n",
    "        self.goal_pos = self.get_random_pos(self.width, self.height)\n",
    "\n",
    "        # Assign agents to random positions\n",
    "        self.agent_positions = [\n",
    "            self.get_random_pos(self.width, self.height) for _ in self.agents\n",
    "        ]\n",
    "\n",
    "    # ----- Core Functions ----- #\n",
    "    def step(self, is_testing=False):\n",
    "        if self.has_ended():\n",
    "            return\n",
    "        if self.idx >= len(self.agents):\n",
    "            self.step_count += 1\n",
    "            self.idx = 0\n",
    "\n",
    "        agent = self.agents[self.idx]\n",
    "        observed_state = self.extract_state(self.idx)\n",
    "\n",
    "        # Disable epsilon exploration when testing or when not being trained\n",
    "        choose_best = is_testing or not agent.get_type() in self.learners\n",
    "        action = agent.choose_action(observed_state, choose_best=choose_best)\n",
    "        reward, next_state, is_terminal = self.move(self.idx, action)\n",
    "\n",
    "        loss, epsilon = agent.update(\n",
    "            state=observed_state,\n",
    "            action=action,\n",
    "            reward=reward,\n",
    "            next_state=next_state,\n",
    "            is_terminal=is_terminal,\n",
    "        )\n",
    "\n",
    "        self.idx += 1\n",
    "        return loss, epsilon\n",
    "\n",
    "    def move(\n",
    "        self, idx: int, action: Tuple[int, int]\n",
    "    ):  # List of actions, in the same order as self.agents\n",
    "        # Update agent to temporary location according to move\n",
    "        old_x, old_y = self.agent_positions[idx]\n",
    "        dx, dy = action\n",
    "        new_x, new_y = old_x + dx, old_y + dy\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        # Penalise when walking into wall\n",
    "        def clamp(i: int, lower: int, upper: int) -> Tuple[int, int]:\n",
    "            penalty = -10 if i < lower or i > upper else 0\n",
    "            return penalty, min(max(i, lower), upper)\n",
    "\n",
    "        # Retreive reward and new location according to Entity.interaction\n",
    "        penalty, new_x = clamp(new_x, 0, self.width - 1)\n",
    "        penalty, new_y = clamp(new_y, 0, self.height - 1)\n",
    "        reward += penalty\n",
    "\n",
    "        # -1 for each turn\n",
    "        reward -= 1\n",
    "\n",
    "        new_pos = new_x, new_y\n",
    "        agent = self.agents[idx]\n",
    "        if new_pos == self.goal_pos:\n",
    "            if agent.have_secret:\n",
    "                # +50 for reaching the goal with secret\n",
    "                reward += 50\n",
    "                self.goal_reached = True\n",
    "        else:\n",
    "            other_indices = [\n",
    "                other_idx\n",
    "                for (other_idx, pos) in enumerate(self.agent_positions)\n",
    "                if other_idx != idx and pos == new_pos\n",
    "            ]\n",
    "            other_agents_diff_type = [\n",
    "                self.agents[o_idx]\n",
    "                for o_idx in other_indices\n",
    "                if self.agents[o_idx].get_type() != self.agents[idx].get_type()\n",
    "            ]\n",
    "            if len(other_agents_diff_type) > 0:\n",
    "                if not self.agents[idx].have_secret:\n",
    "                    # + 20 for getting the secret\n",
    "                    reward += 20\n",
    "                for agent in other_agents_diff_type + [self.agents[idx]]:\n",
    "                    agent.have_secret_(True)\n",
    "\n",
    "        self.agent_positions[idx] = new_pos\n",
    "        return reward, self.extract_state(idx), self.goal_reached\n",
    "\n",
    "    # ----- Private Functions ----- #\n",
    "    def reorder_for_min_step(self):\n",
    "        # Further away an agent is from goal position, earlier it moves\n",
    "        idx_positions = list(enumerate(self.agent_positions))\n",
    "        idx_positions.sort(\n",
    "            key=lambda idx_pos: -self.calc_mht_dist(idx_pos[1], self.goal_pos)\n",
    "        )\n",
    "        self.agents = [self.agents[i] for i, pos in idx_positions]\n",
    "        self.agent_positions = [self.agent_positions[i] for i, pos in idx_positions]\n",
    "\n",
    "    # ----- Public Functions ----- #\n",
    "    def reset(self, random_pos=True):\n",
    "        self.step_count = 1\n",
    "        self.goal_reached = False\n",
    "        self.idx = 0\n",
    "        if random_pos:\n",
    "            self.set_interactive_tiles()\n",
    "        self.min_step = self.calculate_min_step()\n",
    "        for agent in self.agents:\n",
    "            agent.reset()\n",
    "        self.reorder_for_min_step()\n",
    "\n",
    "    def extract_state(self, idx):\n",
    "        if debug:\n",
    "            print(f\"all agent pos: {self.agent_positions}\")\n",
    "            print(f\"all types: {[agent.get_type() for agent in self.agents]}\")\n",
    "\n",
    "        type = self.agents[idx].get_type()\n",
    "        x, y = self.agent_positions[idx]\n",
    "        x2, y2 = self.get_closest_other_agent(x, y, type)\n",
    "        x3, y3 = self.get_goal_positions()\n",
    "\n",
    "        state = torch.zeros(state_size, dtype=dtype)\n",
    "\n",
    "        # One-hot vector of agent_pos, closest_oppo, goal_pos concatenatanted, with one bit for self.have_secret\n",
    "        state[x * side + y] = 1\n",
    "        state[side**2 + x2 * side + y2] = 1\n",
    "        state[side**2 * 2 + x3 * side + y3] = 1\n",
    "        state[side**2 * 3] = 1 if self.agents[idx].have_secret else 0\n",
    "\n",
    "        return state\n",
    "\n",
    "    def get_closest_other_agent(self, x, y, type):\n",
    "        other_type = 2 if type == 1 else 1\n",
    "        min_dist = 1e9\n",
    "        min_x, min_y = -1, -1\n",
    "\n",
    "        for agent, agent_pos in zip(self.agents, self.agent_positions):\n",
    "            if agent.get_type() == other_type:\n",
    "                other_x, other_y = agent_pos\n",
    "                dist = abs(other_x - x) + abs(other_y - y)\n",
    "                if dist < min_dist:\n",
    "                    min_x, min_y = other_x, other_y\n",
    "                    min_dist = dist\n",
    "\n",
    "        if debug:\n",
    "            print(\n",
    "                f\"for current agent {x, y}, closest agent of opposite type of {type} is at {min_x, min_y}\"\n",
    "            )\n",
    "        return min_x, min_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os, numpy as np, torch\n",
    "\n",
    "def seed_all(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_exp = 100\n",
    "upd_freq = 80\n",
    "gamma = 0.9\n",
    "eps_min = 0.005\n",
    "batch_size = 32\n",
    "eps_decay_final_step = 1.99e4\n",
    "max_grad_norm = 5e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = Storage(20000)\n",
    "buffer1, buffer2 = ExpBuffer(max=max_exp), ExpBuffer(max=max_exp)\n",
    "dqn1, dqn2 = DQN(state_size=state_size, action_size=action_size), DQN(state_size=state_size, action_size=action_size)\n",
    "kwargs = {\n",
    "    \"gamma\": gamma,\n",
    "    \"batch_size\": batch_size,\n",
    "}\n",
    "agents = [\n",
    "    Agent1(dqn1, buffer1, **kwargs),\n",
    "    Agent1(dqn1, buffer1, **kwargs),\n",
    "    Agent2(dqn2, buffer2, **kwargs),\n",
    "    Agent2(dqn2, buffer2, **kwargs),\n",
    "]\n",
    "grid = Grid(\n",
    "    side,\n",
    "    side,\n",
    "    agents,\n",
    "    storage,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by training agents with said parameters to demonstrate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.train(\n",
    "    20000,\n",
    "    upd_freq=upd_freq,\n",
    "    eps_min=eps_min,\n",
    "    eps_decay_final_step=eps_decay_final_step,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    dqn1=dqn1,\n",
    "    dqn2=dqn2,\n",
    ")\n",
    "# dqn1.save(\"dqn1\")\n",
    "# dqn2.save(\"dqn2\")\n",
    "Graph(storage=storage, keys=[\"ml_losses\"]).show()\n",
    "Graph(storage=storage, keys=[\"excess_step\", \"epsilon\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.small_test(10000)\n",
    "Graph(storage=storage, keys=[\"excess_step_hist\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation of the trained agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.disable_learning(agent_type=1)\n",
    "grid.disable_learning(agent_type=2)\n",
    "vis = Visualization(storage, grid).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the best performing networks (trained with the correct seed) and run a test of random 10000 scenarrios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn1.load(\"dqn1\")\n",
    "dqn2.load(\"dqn2\")\n",
    "\n",
    "storage.excess_step_hist = Array(\"i\", 51)\n",
    "grid.small_test(10000)\n",
    "Graph(storage=storage, keys=[\"excess_step_hist\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we run a full test on all possible scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage.excess_step_hist = Array(\"i\", 51)\n",
    "grid.test()\n",
    "Graph(storage=storage, keys=[\"step_count_hist\", \"excess_step_hist\"]).show()\n",
    "grid.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Game Thoeretic Analysis**\n",
    "\n",
    "To analyse the scenario in a game-theoretic sense, we simply the grid to be a 1x5 grid and we place only 2 agents. They have the following positions (for simplification, only the x-coordinate is written out):\n",
    "- Goal: 0\n",
    "- Agent_1: 1\n",
    "- Agent_2: 3\n",
    "\n",
    "| Goal | Agent_1 | [Empty] | Agent_2 | [Empty] |\n",
    "|:------|------|--------|------|-------:|\n",
    "\n",
    "And using the reward structure we defined, we have the following payoff matrix, with Agent_1 being the row player and Agent_2 being the column player.\n",
    "\n",
    "|/| Left | Right | Top | Down | Stay Still |\n",
    "|:------|------|--------|-----|--------|-------:|\n",
    "| Left | -1, -1 | -1, -1|-1, -10|-1, -10|-1, -1|\n",
    "| Right | 20, 20 | -1, -1|-1, -10|-1, -10|-1, -1|\n",
    "| Top | -10, -1 | -10, -1|-10, -10|-10, -10|-10, -1|\n",
    "| Down | -10, -1 | -10, -1|-10, -10|-10, -10|-10, -1|\n",
    "| Stay Still | -1, -1 | -1, -1|-1, -10|-1, -10|-1, -1|\n",
    "\n",
    "\n",
    "Since for both players, strategy **Top** and **Down** are always domiated by **Stay Still** we simplify by removing the 2 rows/columns\n",
    "\n",
    "|/| Left | Right | Stay Still |\n",
    "|:------|------|--------|-------:|\n",
    "| Left | -1, -1 | -1, -1|-1, -1|\n",
    "| Right | 20, 20 | -1, -1|-1, -1|\n",
    "| Stay Still | -1, -1 | -1, -1|-1, -1|\n",
    "\n",
    "It then becomes clear (Right, Left) is the pareto optimal outcome as well as the nash equilibrium. At the point, neither player has the incentive to change their strategy.\n",
    "\n",
    "|| Left |\n",
    "|:---------------|--------:|\n",
    "| Right | 20, 20 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If agents were allowed to jump to locations by paying an amount equal to manhattan distance D. This can be generalised into the following form over a longer distance, where D is the manhattan distance bewteen the 2 agents\n",
    "\n",
    "|/| Meetup | Wait |\n",
    "|:------|------|-------:|\n",
    "| Meetup | 20 - D/2, 20 - D/2 | 20 - D, 20 - D|\n",
    "| Wait | 20 - D, 20 - D | $-\\infty$, $-\\infty$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore agents' behaviours are expected to converge to a fair solution where both agents actively move closer and try to meet up at each step. This strategy profile is stable and no agents have the incentive to deviate.\n",
    "\n",
    "By using Q-learning implementation, we can quickly observe the outcome and be confident that eventually it does converge to proposed strategy. Whereas by using game theory, we can understand the reason being the converged strategy, as well as be certain that such convergence is stable and certain in the long run. Certainty, or proof of correctness, is an aspect covered only by theoretical approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
