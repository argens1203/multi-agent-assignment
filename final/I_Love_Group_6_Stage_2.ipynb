{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "side = 4\n",
    "state_size = 4 * 4 * 3\n",
    "device = torch.device(\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "dtype = torch.float32\n",
    "# if device.type == \"mps\" else torch.float64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "# from core import state_size\n",
    "\n",
    "statespace_size = state_size\n",
    "\n",
    "\n",
    "# The function \"prepare_torch\" needs to be called once and only once at the start of your program to initialise PyTorch and generate the two Q-networks. It returns the target model (for testing).\n",
    "def prepare_torch():\n",
    "    global statespace_size\n",
    "    global model, model_hat\n",
    "    global optimizer\n",
    "    global loss_fn\n",
    "    l1 = statespace_size\n",
    "    l2 = 150\n",
    "    l3 = 100\n",
    "    l4 = 4\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(l1, l2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(l2, l3),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(l3, l4),\n",
    "    ).to(device)\n",
    "    model_hat = copy.deepcopy(model).to(device)\n",
    "    model_hat.load_state_dict(model.state_dict())\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    learning_rate = 1e-3\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model_hat\n",
    "\n",
    "\n",
    "# The function \"update_target\" copies the state of the prediction network to the target network. You need to use this in regular intervals.\n",
    "def update_target():\n",
    "    global model, model_hat\n",
    "    model_hat.load_state_dict(model.state_dict())\n",
    "\n",
    "\n",
    "# The function \"get_qvals\" returns a numpy list of qvals for the state given by the argument _based on the prediction network_.\n",
    "def get_qvals(state):\n",
    "    return model(state).to(device)\n",
    "\n",
    "\n",
    "# The function \"get_maxQ\" returns the maximum q-value for the state given by the argument _based on the target network_.\n",
    "def get_maxQ(s):\n",
    "    maxxed = torch.max(model_hat(s), dim=1)\n",
    "    return torch.max(model_hat(s), dim=1).values.float()\n",
    "\n",
    "\n",
    "# The function \"train_one_step_new\" performs a single training step. It returns the current loss (only needed for debugging purposes). Its parameters are three parallel lists: a minibatch of states, a minibatch of actions, a minibatch of the corresponding TD targets and the discount factor.\n",
    "def train_one_step(states, actions, targets):\n",
    "    # print(states)\n",
    "    # print(states.shape)\n",
    "    # for s in states:\n",
    "    # print(s.shape)\n",
    "    # pass to this function: state1_batch, action_batch, TD_batch\n",
    "    global model, model_hat\n",
    "    # state1_batch = torch.cat([torch.from_numpy(s).float() for s in states])\n",
    "    state1_batch = states.to(device)\n",
    "    action_batch = actions.to(device)\n",
    "    # print(action_batch.shape)\n",
    "    # print(state1_batch.shape)\n",
    "    Q1 = model(state1_batch)\n",
    "    X = Q1.gather(dim=1, index=action_batch.long().unsqueeze(dim=1)).squeeze()\n",
    "    Y = targets.clone().detach().to(device).float()\n",
    "    loss = loss_fn(X, Y)\n",
    "    # print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item() / len(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "class ExpBuffer:\n",
    "    def __init__(self):\n",
    "        self.max = 1000\n",
    "        self.itr = 0\n",
    "        self.has_reached = False\n",
    "\n",
    "        self.states = torch.empty((self.max, state_size), dtype=dtype)\n",
    "        self.actions = torch.empty((self.max,), dtype=dtype)\n",
    "        self.rewards = torch.empty((self.max,), dtype=dtype)\n",
    "        self.next_states = torch.empty((self.max, state_size), dtype=dtype)\n",
    "        self.is_terminals = torch.empty((self.max,), dtype=torch.bool)\n",
    "        pass\n",
    "\n",
    "    def insert(self, state, action, reward, next_state, is_terminal):\n",
    "        self.itr %= self.max\n",
    "        self.states[self.itr] = state\n",
    "        self.actions[self.itr] = action\n",
    "        self.rewards[self.itr] = reward\n",
    "        self.next_states[self.itr] = next_state\n",
    "        self.is_terminals[self.itr] = is_terminal\n",
    "        self.itr += 1\n",
    "\n",
    "        if self.itr >= self.max:\n",
    "            self.has_reached = True\n",
    "\n",
    "    def extract(self, batch_size) -> Tuple[List[List[int]], List[int], List[float]]:\n",
    "        indices = np.random.randint(\n",
    "            0, self.max if self.has_reached else self.itr, batch_size\n",
    "        )\n",
    "        # print(len(self.states))\n",
    "        # print(indices)\n",
    "        return (\n",
    "            self.states[indices],\n",
    "            self.actions[indices],\n",
    "            self.rewards[indices],\n",
    "            self.next_states[indices],\n",
    "            self.is_terminals[indices],\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, idx, all_states, actions):\n",
    "        # Agent property (for illustration purposes)\n",
    "        self.is_having_item = False\n",
    "        self.total_reward = 0\n",
    "\n",
    "        self.actions = actions  # TODO: encode different action for different state. How to initialize Q-Table\n",
    "        self.idx = idx\n",
    "\n",
    "        # Initialize Q Table for all state-action to be 0\n",
    "        # TODO: use multi-D np array\n",
    "        self.min_buffer = 200\n",
    "        self.step_count = 0\n",
    "        self.C = 500\n",
    "        self.Q = np.zeros((all_states, len(actions)))\n",
    "\n",
    "        # Initialize Learning param\n",
    "        # TODO: fix resetting epsilon\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.997  # TODO: reduce the decay (ie. increase the number)\n",
    "        self.epsilon_min = 0.1\n",
    "        self.gamma = 0.997\n",
    "\n",
    "        # self.alpha = 0.1\n",
    "\n",
    "        self.buffer = ExpBuffer()\n",
    "\n",
    "        prepare_torch()\n",
    "\n",
    "    # ----- Core Functions ----- #\n",
    "    def choose_action(self, state: \"State\", explore=True):\n",
    "        if explore and np.random.rand() < self.epsilon:\n",
    "            return random.choice(self.actions)\n",
    "        else:\n",
    "            # Extract immutable state information\n",
    "            state_i = self.massage(state)\n",
    "            idx = torch.argmax(get_qvals(state_i))\n",
    "            return self.actions[idx]\n",
    "\n",
    "    def update_learn(\n",
    "        self,\n",
    "        state: \"State\",\n",
    "        action: \"Action\",\n",
    "        reward: int,\n",
    "        next_state: \"State\",\n",
    "        is_terminal: bool,\n",
    "        learn=True,\n",
    "    ):\n",
    "        self.update(next_state, reward)\n",
    "\n",
    "        # Extract immutable state information\n",
    "        # nxt_state_i = self.massage(next_state)\n",
    "\n",
    "        if not learn:\n",
    "            return\n",
    "\n",
    "        # # All states (including terminal states) have initial Q-values of 0 and thus there is no need for branching for handling terminal next state\n",
    "        # self.Q[state_i][self.actions.index(action)] += self.alpha * (\n",
    "        #     reward\n",
    "        #     + self.gamma * np.max(self.Q[nxt_state_i])\n",
    "        #     - self.Q[state_i][self.actions.index(action)]\n",
    "        # )\n",
    "\n",
    "        state_i = self.massage(state)\n",
    "        nxt_state_i = self.massage(next_state)\n",
    "        # target_val = self.gamma * get_maxQ(nxt_state_i) + reward\n",
    "        # if is_terminal:\n",
    "        #     target_val = torch.tensor(reward)\n",
    "\n",
    "        # next_qa = np.copy(current_qa)\n",
    "        # next_qa[np.argmax(next_qa)] = target_val\n",
    "        self.buffer.insert(\n",
    "            state_i, self.actions.index(action), reward, nxt_state_i, is_terminal\n",
    "        )\n",
    "        if len(self.buffer) >= self.min_buffer:\n",
    "            states, actions, rewards, next_states, is_terminals = self.buffer.extract(\n",
    "                200\n",
    "            )\n",
    "            rewards = rewards.to(device)\n",
    "            indices = is_terminals.nonzero().to(device)\n",
    "            targets = self.gamma * get_maxQ(next_states.to(device)) + rewards\n",
    "            targets[indices] = rewards[\n",
    "                indices\n",
    "            ]  # For terminal states, target_val is reward\n",
    "            # print(states, actions, targets)\n",
    "            # print(states.shape, actions.shape, targets.shape)\n",
    "            loss = train_one_step(states, actions, targets)\n",
    "\n",
    "        if self.step_count >= self.C:\n",
    "            update_target()\n",
    "            self.step_count = 0\n",
    "        else:\n",
    "            self.step_count += 1\n",
    "\n",
    "        # Epsilon decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        return loss\n",
    "\n",
    "    # ----- Public Functions ----- #\n",
    "    def has_item(self):\n",
    "        return self.is_having_item\n",
    "\n",
    "    def get_total_reward(self):\n",
    "        return self.total_reward\n",
    "\n",
    "    def update(self, state: \"State\", reward=0):\n",
    "        self.is_having_item = state.item_taken()\n",
    "        self.total_reward += reward\n",
    "\n",
    "    def reset(self):\n",
    "        self.is_having_item = False\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def get_q_table(self):\n",
    "        return self.Q\n",
    "\n",
    "    # ----- Private Functions ----- #\n",
    "    # Extract immutable information from State object\n",
    "    def massage(self, state: \"State\"):\n",
    "        state_i = state.extract_state(self.idx).to(device).float()\n",
    "        return state_i + 0.001  # 5 Minutes\n",
    "        # return state_i + torch.rand(state_size).to(device) / 100.0  # 15 Minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, TYPE_CHECKING\n",
    "\n",
    "class Cell:\n",
    "    def __init__(self, pos):\n",
    "        x, y = pos\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    # returns: score delta, (new_coordinate_x, new_coordinate_y)\n",
    "    def interact(self, other: \"Agent\") -> Tuple[int, Tuple[int, int]]:\n",
    "        return -1, (self.x, self.y)\n",
    "\n",
    "    def __copy__(self):\n",
    "        return Cell((self.x, self.y))\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        return self.__copy__()\n",
    "\n",
    "\n",
    "class Goal(Cell):\n",
    "    def __init__(self, pos):\n",
    "        x, y = pos\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.reached = False\n",
    "\n",
    "    def interact(self, other: \"Agent\"):\n",
    "        if other.has_item() and not self.reached:\n",
    "            self.reached = True\n",
    "            return 50, (self.x, self.y)\n",
    "        else:\n",
    "            return -1, (self.x, self.y)\n",
    "\n",
    "    def has_reached(self):\n",
    "        return self.reached\n",
    "\n",
    "    def __copy__(self):\n",
    "        copy = Goal((self.x, self.y))\n",
    "        copy.reached = self.reached\n",
    "        return copy\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        return self.__copy__()\n",
    "\n",
    "\n",
    "class Item(Cell):\n",
    "    def __init__(self, pos):\n",
    "        self.taken = False\n",
    "        x, y = pos\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def interact(self, other: \"Agent\"):\n",
    "        if not self.taken and not other.has_item():\n",
    "            # agent update is done in agent.update(...)\n",
    "            self.taken = True\n",
    "            return 50, (self.x, self.y)\n",
    "\n",
    "        return -1, (self.x, self.y)\n",
    "\n",
    "    def get_pos(self):\n",
    "        return self.x, self.y\n",
    "\n",
    "    def __copy__(self):\n",
    "        copy = Item((self.x, self.y))\n",
    "        copy.taken = self.taken\n",
    "        return copy\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        return self.__copy__()\n",
    "\n",
    "\n",
    "class Wall(Cell):\n",
    "    def __init__(self, pos, dimensions):\n",
    "        x, y = pos\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "        width, height = dimensions\n",
    "        self.new_x = min(width - 1, max(0, x))\n",
    "        self.new_y = min(height - 1, max(0, y))\n",
    "\n",
    "    def interact(self, other: \"Agent\"):\n",
    "        return -10, (self.new_x, self.new_y)\n",
    "\n",
    "    def __copy__(self):\n",
    "        return Wall((self.x, self.y))\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        return self.__copy__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING, List, Tuple, Dict, Set\n",
    "\n",
    "import random\n",
    "\n",
    "class GridFactory:\n",
    "    # Getting a random location in a grid, excluding certain locations\n",
    "    def get_random_pos(\n",
    "        width: int, height: int, exclude: List[Tuple[int, int]] = []\n",
    "    ) -> Tuple[int, int]:\n",
    "        while True:\n",
    "            position = (\n",
    "                random.randint(0, width - 1),\n",
    "                random.randint(0, height - 1),\n",
    "            )\n",
    "            if position not in exclude:\n",
    "                return position\n",
    "\n",
    "\n",
    "class Grid:\n",
    "    def __init__(self, width: int, height: int):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.max_reward = 0\n",
    "\n",
    "        self.state: Dict[Tuple[int, int], Cell] = (\n",
    "            {}\n",
    "        )  # TODO: multiple entities in one cell\n",
    "        self.lookup: set[Cell] = set()  # Interactive tiles\n",
    "        self.agents: List[\"Agent\"] = []\n",
    "        self.agent_positions: List[Tuple[int, int]] = []\n",
    "\n",
    "        self.init_environment()\n",
    "\n",
    "    # ----- Init Functions ----- #\n",
    "    def init_environment(self):\n",
    "        for x in range(-1, self.width + 1):\n",
    "            for y in range(-1, self.height + 1):\n",
    "                if x < 0 or x >= self.width:\n",
    "                    self.state[(x, y)] = Wall((x, y), (self.width, self.height))\n",
    "                elif y < 0 or y >= self.height:\n",
    "                    self.state[(x, y)] = Wall((x, y), (self.width, self.height))\n",
    "                else:\n",
    "                    self.state[(x, y)] = Cell((x, y))\n",
    "\n",
    "    # ----- Core Functions ----- #\n",
    "    def move(\n",
    "        self, actions: List[\"Action\"]\n",
    "    ):  # List of actions, in the same order as self.agents\n",
    "        # Update agent to temporary location according to move\n",
    "        temp_positions = [\n",
    "            self.process_action(action, agent_pos)\n",
    "            for action, agent_pos in zip(actions, self.agent_positions)\n",
    "        ]\n",
    "\n",
    "        # Retreive reward and new location according to Entity.interaction\n",
    "        reward_new_positions = [\n",
    "            self.state[(x, y)].interact(agent)\n",
    "            for agent, (x, y) in zip(self.agents, temp_positions)\n",
    "        ]\n",
    "        rewards, new_positions = zip(*reward_new_positions)\n",
    "\n",
    "        # Update new positions\n",
    "        self.agent_positions = new_positions\n",
    "\n",
    "        # Return move results, in the same order as self.agents\n",
    "        return [\n",
    "            (reward, self.get_state(), self.get_state().is_terminal())\n",
    "            for reward in rewards\n",
    "        ]\n",
    "\n",
    "    # ----- Private Functions ----- #\n",
    "    def process_action(\n",
    "        self, action: List[\"Action\"], agent_position: List[Tuple[int, int]]\n",
    "    ):\n",
    "        # Move according to action\n",
    "        x, y = agent_position\n",
    "        dx, dy = self.interpret_action(action)\n",
    "        return x + dx, y + dy\n",
    "\n",
    "    def interpret_action(self, action: \"Action\"):\n",
    "        if action == Action.NORTH:\n",
    "            return 0, -1\n",
    "        if action == Action.SOUTH:\n",
    "            return 0, 1\n",
    "        if action == Action.EAST:\n",
    "            return 1, 0\n",
    "        if action == Action.WEST:\n",
    "            return -1, 0\n",
    "\n",
    "    def set_interactive_tiles(self):\n",
    "        self.lookup.clear()\n",
    "        used_pos = []\n",
    "\n",
    "        # TODO: extract repeated code\n",
    "\n",
    "        # Assign goal to set position\n",
    "        goal_pos = GridFactory.get_random_pos(self.width, self.height, used_pos)\n",
    "        # goal_pos = (self.width - 1, self.height - 1)\n",
    "        goal = Goal(goal_pos)\n",
    "        self.state[goal_pos] = goal\n",
    "        self.lookup.add(goal)\n",
    "        used_pos.append(goal_pos)\n",
    "\n",
    "        # Assign items to a random position in the remaining tiles\n",
    "        item_pos = GridFactory.get_random_pos(self.width, self.height, used_pos)\n",
    "        item = Item(item_pos)\n",
    "        self.state[item_pos] = item\n",
    "        self.lookup.add(item)\n",
    "        used_pos.append(item_pos)\n",
    "\n",
    "        # Assign agents to random positions\n",
    "        self.agent_positions = []\n",
    "        for _ in self.agents:\n",
    "            agent_pos = GridFactory.get_random_pos(self.width, self.height, used_pos)\n",
    "            used_pos.append(agent_pos)\n",
    "            self.agent_positions.append(agent_pos)\n",
    "\n",
    "        # Future proofing: update agents in case they spwaned on an item\n",
    "        for agent in self.agents:\n",
    "            agent.update(State(self.agent_positions, self.lookup))\n",
    "\n",
    "        self.max_reward = GridUtil.calculate_max_reward(self)\n",
    "\n",
    "    # ----- Public Functions ----- #\n",
    "    def reset(self):\n",
    "        self.init_environment()\n",
    "        self.set_interactive_tiles()\n",
    "\n",
    "    def add_agent(self, agent: \"Agent\"):\n",
    "        self.agents.append(agent)\n",
    "\n",
    "    def get_state(self):\n",
    "        return State(self.agent_positions, self.lookup)\n",
    "\n",
    "    def get_size(self):\n",
    "        return self.width, self.height\n",
    "\n",
    "    def get_max_reward(self):\n",
    "        return self.max_reward\n",
    "\n",
    "\n",
    "class GridUtil:\n",
    "    def calculate_max_reward(grid: Grid):\n",
    "        # TODO: can only work with one agent and one item ATM\n",
    "        x1, y1 = grid.get_state().get_agent_positions()[0]\n",
    "        x2, y2 = grid.get_state().get_item_positions()[0]\n",
    "        x3, y3 = grid.get_state().get_goal_positions()\n",
    "\n",
    "        # Manhanttan distance from agent to obj and obj to goal\n",
    "        dist_to_obj = abs(x1 - x2) + abs(y1 - y2)\n",
    "        dist_to_goal = abs(x2 - x3) + abs(y2 - y3)\n",
    "\n",
    "        # +100 for reward and +2 for 2 unneeded mark deduction when stepping on item and goal respectively\n",
    "        return (dist_to_obj + dist_to_goal) * -1 + 102\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action:\n",
    "    NORTH = \"N\"\n",
    "    WEST = \"W\"\n",
    "    EAST = \"E\"\n",
    "    SOUTH = \"S\"\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "class State:\n",
    "    def __init__(self, agent_positions, lookup):\n",
    "        self.agent_positions = agent_positions\n",
    "        self.lookup = deepcopy(lookup)\n",
    "\n",
    "    def get_possible_actions():\n",
    "        # Generate possible actions\n",
    "        return [Action.NORTH, Action.SOUTH, Action.EAST, Action.WEST]\n",
    "\n",
    "    # TODO: fix hardcode\n",
    "    def get_possible_states(width, height):\n",
    "        # Generate all possible states\n",
    "        return state_size\n",
    "        positions = [(x, y) for x in range(width) for y in range(height)]\n",
    "        has_items = [True, False]\n",
    "        return itertools.product(positions, positions, has_items)\n",
    "\n",
    "    # ----- Private Functions ----- #\n",
    "    def get_goal(self) -> \"Goal\":\n",
    "        return next((x for x in self.lookup if isinstance(x, Goal)), [None])\n",
    "\n",
    "    def get_items(self):\n",
    "        return [x for x in self.lookup if isinstance(x, Item)]\n",
    "\n",
    "    def get_item_positions(self):\n",
    "        return [item.get_pos() for item in self.get_items()]\n",
    "\n",
    "    # TODO: fix hardcode\n",
    "    def item_taken(self):\n",
    "        item = next((x for x in self.lookup if isinstance(x, Item)), [None])\n",
    "        return item.taken\n",
    "\n",
    "    def extract_state(self, idx):\n",
    "        x, y = self.agent_positions[idx]\n",
    "        x2, y2 = self.get_item_positions()[0]\n",
    "        x3, y3 = self.get_goal_positions()\n",
    "        # print(x, y, x2, y2, x3, y3)\n",
    "        # TODO: remove hardcoded item_pos indices\n",
    "        # return agent_pos, item_pos[0], self.has_item()\n",
    "        state = torch.empty(state_size, dtype=dtype)\n",
    "        state[x * side + y] = 1\n",
    "        if not self.item_taken():\n",
    "            state[side**2 + x2 * side + y2] = 1\n",
    "        state[side**2 * 2 + x3 * side + y3] = 1\n",
    "\n",
    "        return state\n",
    "\n",
    "    # ----- Information Extraction ----- #\n",
    "    def get_agent_positions(self):\n",
    "        return self.agent_positions\n",
    "\n",
    "    # TODO: fix hardcode\n",
    "    def get_goal_positions(self):\n",
    "        goal = self.get_goal()\n",
    "        return goal.x, goal.y\n",
    "\n",
    "    def get_item_positions(self):\n",
    "        return [item.get_pos() for item in self.get_items()]\n",
    "\n",
    "    # TODO: fix hardcode\n",
    "    def is_terminal(self):\n",
    "        goal = self.get_goal()\n",
    "        return goal.has_reached()\n",
    "\n",
    "    def get_untaken_item_pos(self):\n",
    "        untaken_items = filter(lambda i: not i.taken, self.get_items())\n",
    "        return [i.get_pos() for i in untaken_items]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Array\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Storage:\n",
    "    def __init__(self, max_itr: int):\n",
    "        self.itr = 0\n",
    "        self.max_itr = max_itr\n",
    "\n",
    "        # TODO: max_itr could be dynamic\n",
    "        self.iterations = Array(\"i\", range(max_itr))\n",
    "        self.losses = Array(\"i\", max_itr)\n",
    "        self.epsilon = Array(\"f\", max_itr)\n",
    "        self.test_loss = []\n",
    "        self.ml_losses = []\n",
    "\n",
    "    def reset_counter(self):\n",
    "        self.itr = 0\n",
    "\n",
    "    def append_loss_epsilon(self, loss: int, epsilon: float):\n",
    "        if self.itr >= self.max_itr:\n",
    "            self.itr = 0\n",
    "        self.losses[self.itr] = loss\n",
    "        self.epsilon[self.itr] = epsilon\n",
    "        self.itr += 1\n",
    "\n",
    "    def append_test_loss(self, test_loss: int):\n",
    "        self.test_loss.append(test_loss)\n",
    "\n",
    "    def reset_test_loss(self):\n",
    "        self.test_loss = []\n",
    "\n",
    "    def append_ml_losses(self, ml_losses: List[float]):\n",
    "        self.ml_losses += ml_losses\n",
    "\n",
    "    def get_all(self):\n",
    "        return self.iterations, self.losses, self.epsilon, self.test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING, List\n",
    "import datetime\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, max_itr):\n",
    "        self.max_itr = max_itr\n",
    "\n",
    "    def bind(\n",
    "        self, model: \"Model\", storage: \"Storage\", grid: \"Grid\", agents: List[\"Agent\"]\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.storage = storage\n",
    "        self.grid = grid\n",
    "        self.agents = agents\n",
    "\n",
    "    def train(self, itr=1):\n",
    "        start = datetime.datetime.now()\n",
    "        print(f\"Start Time: {start}\")\n",
    "        self.model.reset()\n",
    "        for i in range(itr):\n",
    "            (loss, reward, epsilon, ml_losses) = self.train_one_game()\n",
    "            # self.storage.append_loss_epsilon(loss, epsilon)\n",
    "\n",
    "            self.storage.append_ml_losses(ml_losses)\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(\n",
    "                    f\"Epoch: {i+1}/{itr} -- Time Elapsed: {datetime.datetime.now() - start}\"\n",
    "                )\n",
    "        return self.storage.ml_losses\n",
    "\n",
    "    def test(self, itr=1):\n",
    "        self.model.reset()\n",
    "        self.storage.reset_test_loss()\n",
    "        # for i in range(self.max_itr):\n",
    "        #     self.storage.test_loss[i] = 0\n",
    "        for _ in range(itr):\n",
    "            (loss, reward, epsilon, _) = self.train_one_game(learn=False)\n",
    "            # self.storage.append_test_loss(loss)\n",
    "            self.storage.append_test_loss(loss)\n",
    "        return self.storage.test_loss\n",
    "\n",
    "    def train_one_game(self, learn=True):\n",
    "        self.model.reset()\n",
    "        max_reward = self.grid.get_max_reward()\n",
    "\n",
    "        max_step_count = 50 if learn else 50\n",
    "        step_count = 0\n",
    "        ml_losses = []\n",
    "        while not self.grid.get_state().is_terminal() and step_count < max_step_count:\n",
    "            ml_loss = self.step(learn)\n",
    "            if ml_loss is not None:\n",
    "                ml_losses.append(ml_loss)\n",
    "            step_count += 1\n",
    "\n",
    "        total_reward = sum(map(lambda a: a.get_total_reward(), self.agents))\n",
    "        loss = max_reward - total_reward\n",
    "        return loss, total_reward, self.agents[0].epsilon, ml_losses  # TODO: 0\n",
    "\n",
    "    def step(self, learn=True):\n",
    "        if self.grid.get_state().is_terminal():\n",
    "            return\n",
    "\n",
    "        state = self.grid.get_state()\n",
    "        actions = [agent.choose_action(state, explore=learn) for agent in self.agents]\n",
    "        results = self.grid.move(actions)\n",
    "        loss = None\n",
    "\n",
    "        for action, (reward, next_state, terminal), agent in zip(\n",
    "            actions, results, self.agents\n",
    "        ):\n",
    "            if learn:\n",
    "                loss = agent.update_learn(state, action, reward, next_state, terminal)\n",
    "            else:\n",
    "                agent.update(next_state, reward)\n",
    "        return loss\n",
    "\n",
    "    def test_in_background(self, ep=1000):\n",
    "        gp, tp = get_test_process(self.storage, self, ep)\n",
    "        gp.start()\n",
    "        tp.start()\n",
    "        gp.join()\n",
    "        tp.join()\n",
    "\n",
    "    def train_in_background(self):\n",
    "        gp, tp, conn1 = get_process(self.storage, self)\n",
    "        gp.start()\n",
    "        tp.start()\n",
    "        gp.join()\n",
    "        tp.join()\n",
    "\n",
    "        name = conn1.recv()\n",
    "        # TODO: return array of trained_Q\n",
    "        trained_Q = get_np_from_name(name)\n",
    "        return trained_Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller:\n",
    "    # Iterate by number of games\n",
    "    def __init__(self):\n",
    "        self.auto_reset = True\n",
    "\n",
    "    def bind(self, model: \"Model\"):\n",
    "        self.model = model\n",
    "        return self\n",
    "\n",
    "    def add_helper(self, storage: \"Storage\", trainer: \"Trainer\"):\n",
    "        self.storage = storage\n",
    "        self.trainer = trainer\n",
    "        return self\n",
    "\n",
    "    def toggle_auto_reset(self):\n",
    "        self.auto_reset = not self.auto_reset\n",
    "        return self.auto_reset\n",
    "\n",
    "    def next(self):\n",
    "        if self.model.has_ended() and self.auto_reset:\n",
    "            self.model.reset()\n",
    "        self.trainer.step(learn=False)\n",
    "        return\n",
    "\n",
    "    def train(self, itr=1):\n",
    "        return self.trainer.train(itr)\n",
    "\n",
    "    def test(self, itr=1):\n",
    "        return self.trainer.test(itr)\n",
    "\n",
    "    def reset(self):\n",
    "        self.model.reset()\n",
    "\n",
    "    def get_metrics(self):\n",
    "        itrs, losses, epsilons, test_losses = self.storage.get_all()\n",
    "        return itrs, losses, epsilons\n",
    "\n",
    "    def test_in_background(self, ep=1000):\n",
    "        self.trainer.test_in_background(ep)\n",
    "\n",
    "    def train_in_background(self):\n",
    "        trained_Q = self.trainer.train_in_background()\n",
    "        # TODO: fix hardcode\n",
    "        self.model.agents[0].Q = trained_Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self, storage: \"Storage\", fig, axs):\n",
    "        self.storage = storage\n",
    "        self.fig = fig\n",
    "        self.ax1, self.ax2 = axs\n",
    "\n",
    "        self.storage = storage\n",
    "        self.ani = animation.FuncAnimation(\n",
    "            self.fig, self.draw, frames=self.frames, interval=100, save_count=100\n",
    "        )\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def frames(self):\n",
    "        while True:\n",
    "            yield None\n",
    "\n",
    "    # Compulsory unused argument\n",
    "    def draw(self, args):\n",
    "        self.plot_losses(\n",
    "            self.ax1,\n",
    "            self.storage.iterations,\n",
    "            self.storage.losses,\n",
    "        )\n",
    "        self.plot_epsilon(\n",
    "            self.ax2,\n",
    "            self.storage.iterations,\n",
    "            self.storage.epsilon,\n",
    "        )\n",
    "\n",
    "    def plot_losses(self, ax, iterations, loss):\n",
    "        # Plotting the loss in the first subplot\n",
    "        ax.plot(iterations, loss, color=\"blue\", label=\"Loss\")\n",
    "        ax.set_title(\"Loss\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "\n",
    "    def plot_epsilon(self, ax, iterations, epsilon):\n",
    "        # Plotting the loss in the first subplot\n",
    "        ax.plot(iterations, epsilon, color=\"blue\", label=\"Loss\")\n",
    "        ax.set_title(\"Epsilon\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.set_ylabel(\"Epsilon\")\n",
    "\n",
    "\n",
    "class TestGraph:\n",
    "    def __init__(self, controller, fig, ax):\n",
    "        self.controller = controller\n",
    "        self.fig = fig\n",
    "        self.ax = ax\n",
    "\n",
    "        self.controller = controller\n",
    "        self.ani = animation.FuncAnimation(\n",
    "            self.fig, self.draw, frames=self.frames, interval=100, save_count=100\n",
    "        )\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def frames(self):\n",
    "        while True:\n",
    "            yield None\n",
    "\n",
    "    def draw(self, args):\n",
    "        self.plot_losses(\n",
    "            self.ax,\n",
    "            self.controller.iterations,\n",
    "            self.controller.test_loss,\n",
    "        )\n",
    "\n",
    "    def plot_losses(self, ax, iterations, loss):\n",
    "        # Plotting the loss in the first subplot\n",
    "        ax.plot(iterations, loss, color=\"blue\", label=\"Loss\")\n",
    "        ax.set_title(\"Loss\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "\n",
    "\n",
    "class MLGraph:\n",
    "    def __init__(self, ml_losses, fig, ax):\n",
    "        ax.plot(range(len(ml_losses)), ml_losses, label=\"Loss\")\n",
    "        # ax.set_title(\"Loss\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        pass\n",
    "\n",
    "    def show(self):\n",
    "\n",
    "        # Display the plots\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from typing import TYPE_CHECKING\n",
    "from multiprocessing import Process, shared_memory, Pipe\n",
    "\n",
    "def draw_graphs(storage: \"Storage\"):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    graph = Graph(storage, fig, axs)\n",
    "\n",
    "\n",
    "def train(trainer: \"Trainer\", connection, ep):\n",
    "    trainer.train(ep)\n",
    "    # TODO: remove hardcode\n",
    "    q = trainer.model.agents[0].get_q_table()\n",
    "\n",
    "    shm = shared_memory.SharedMemory(create=True, size=q.nbytes)\n",
    "    b = np.ndarray(q.shape, dtype=q.dtype, buffer=shm.buf)\n",
    "    b[:] = q[:]\n",
    "    connection.send(shm.name)\n",
    "    shm.close()\n",
    "\n",
    "\n",
    "def get_process(storage: \"Storage\", trainer: \"Trainer\"):\n",
    "    conn1, conn2 = Pipe()\n",
    "    graph_p = Process(\n",
    "        target=draw_graphs,\n",
    "        args=[\n",
    "            storage,\n",
    "        ],\n",
    "    )\n",
    "    # TODO: remove hardcode\n",
    "    train_p = Process(target=train, args=[trainer, conn2, 1000])\n",
    "    return graph_p, train_p, conn1\n",
    "\n",
    "\n",
    "def test(trainer: \"Trainer\", ep):\n",
    "    trainer.test(ep)\n",
    "\n",
    "\n",
    "def draw_test_graph(storage: \"Storage\"):\n",
    "    fig, axs = plt.subplots()\n",
    "    graph = TestGraph(storage, fig, axs)\n",
    "\n",
    "\n",
    "def get_test_process(storage: \"Storage\", trainer: \"Trainer\", ep=1000):\n",
    "    graph_p = Process(\n",
    "        target=draw_test_graph,\n",
    "        args=[storage],\n",
    "    )\n",
    "    test_p = Process(target=test, args=[trainer, ep])\n",
    "    return graph_p, test_p\n",
    "\n",
    "\n",
    "def get_np_from_name(name):\n",
    "    existing_shm = shared_memory.SharedMemory(name=name)\n",
    "    # TODO: remove hardcode\n",
    "    q = np.ndarray((5**5, 4), buffer=existing_shm.buf)\n",
    "    s = np.copy(q)\n",
    "    existing_shm.close()\n",
    "    existing_shm.unlink()\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self, storage: \"Storage\", fig, axs):\n",
    "        self.storage = storage\n",
    "        self.fig = fig\n",
    "        self.ax1, self.ax2 = axs\n",
    "\n",
    "        self.storage = storage\n",
    "        self.ani = animation.FuncAnimation(\n",
    "            self.fig, self.draw, frames=self.frames, interval=100, save_count=100\n",
    "        )\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def frames(self):\n",
    "        while True:\n",
    "            yield None\n",
    "\n",
    "    # Compulsory unused argument\n",
    "    def draw(self, args):\n",
    "        self.plot_losses(\n",
    "            self.ax1,\n",
    "            self.storage.iterations,\n",
    "            self.storage.losses,\n",
    "        )\n",
    "        self.plot_epsilon(\n",
    "            self.ax2,\n",
    "            self.storage.iterations,\n",
    "            self.storage.epsilon,\n",
    "        )\n",
    "\n",
    "    def plot_losses(self, ax, iterations, loss):\n",
    "        # Plotting the loss in the first subplot\n",
    "        ax.plot(iterations, loss, color=\"blue\", label=\"Loss\")\n",
    "        ax.set_title(\"Loss\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "\n",
    "    def plot_epsilon(self, ax, iterations, epsilon):\n",
    "        # Plotting the loss in the first subplot\n",
    "        ax.plot(iterations, epsilon, color=\"blue\", label=\"Loss\")\n",
    "        ax.set_title(\"Epsilon\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.set_ylabel(\"Epsilon\")\n",
    "\n",
    "\n",
    "class TestGraph:\n",
    "    def __init__(self, controller, fig, ax):\n",
    "        self.controller = controller\n",
    "        self.fig = fig\n",
    "        self.ax = ax\n",
    "\n",
    "        self.controller = controller\n",
    "        self.ani = animation.FuncAnimation(\n",
    "            self.fig, self.draw, frames=self.frames, interval=100, save_count=100\n",
    "        )\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def frames(self):\n",
    "        while True:\n",
    "            yield None\n",
    "\n",
    "    def draw(self, args):\n",
    "        self.plot_losses(\n",
    "            self.ax,\n",
    "            self.controller.iterations,\n",
    "            self.controller.test_loss,\n",
    "        )\n",
    "\n",
    "    def plot_losses(self, ax, iterations, loss):\n",
    "        # Plotting the loss in the first subplot\n",
    "        ax.plot(iterations, loss, color=\"blue\", label=\"Loss\")\n",
    "        ax.set_title(\"Loss\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "\n",
    "\n",
    "class MLGraph:\n",
    "    def __init__(self, ml_losses, fig, ax):\n",
    "        ax.plot(range(len(ml_losses)), ml_losses, label=\"Loss\")\n",
    "        # ax.set_title(\"Loss\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        pass\n",
    "\n",
    "    def show(self):\n",
    "\n",
    "        # Display the plots\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from matplotlib.widgets import Button\n",
    "from typing import Tuple, TypeAlias, TYPE_CHECKING\n",
    "\n",
    "Coordinates: TypeAlias = Tuple[float, float, float, float]\n",
    "\n",
    "class Visualization:\n",
    "    def __init__(self, fig, ax):\n",
    "        self.fig = fig\n",
    "        self.ax = ax\n",
    "        self.animating = False\n",
    "\n",
    "    def bind(self, model: \"Model\", controller: \"Controller\", storage: \"Storage\"):\n",
    "        self.model = model\n",
    "        self.controller = controller\n",
    "        self.storage = storage\n",
    "        return self\n",
    "\n",
    "    def show(self):\n",
    "        assert self.model is not None\n",
    "        assert self.controller is not None\n",
    "\n",
    "        self.add_ui_elements()\n",
    "        self.fig.canvas.mpl_connect(\"close_event\", self.on_close)\n",
    "        self.ani = animation.FuncAnimation(\n",
    "            self.fig, self.draw, frames=self.frames, interval=200, save_count=100\n",
    "        )\n",
    "        self.animating = True\n",
    "        plt.show()\n",
    "\n",
    "    def get_info(self):\n",
    "        info = self.model.get_agent_info()\n",
    "        items = self.model.get_untaken_items()\n",
    "        tot_reward = self.model.get_total_reward()\n",
    "        max_reward = self.model.get_max_reward()\n",
    "        return info, items, tot_reward, max_reward\n",
    "\n",
    "    def frames(self):\n",
    "        while True:\n",
    "            if self.animating:\n",
    "                self.controller.next()\n",
    "                yield self.get_info()\n",
    "            else:\n",
    "                yield self.get_info()\n",
    "\n",
    "    # ----- ----- ----- ----- Drawing Functions  ----- ----- ----- ----- #\n",
    "\n",
    "    def draw(self, args):\n",
    "        info, items, tot_reward, max_reward = args\n",
    "\n",
    "        self.ax.clear()\n",
    "        self.draw_grid()\n",
    "        self.draw_agent(info)\n",
    "        self.draw_item(items)\n",
    "\n",
    "        self.reward.set_text(f\"Reward: {tot_reward}\")\n",
    "        self.max_reward.set_text(f\"Max Reward: {max_reward}\")\n",
    "\n",
    "        # Check if the environment is terminal\n",
    "        if self.model.has_ended():\n",
    "            self.draw_complete()\n",
    "\n",
    "        # Early return if animating, since animation automatically refreshes canvas\n",
    "        if self.animating:\n",
    "            return\n",
    "\n",
    "        self.fig.canvas.draw()\n",
    "\n",
    "    def draw_grid(self):\n",
    "        width, height = self.model.get_size()\n",
    "        for x in range(width):\n",
    "            for y in range(height):\n",
    "                rect = patches.Rectangle(\n",
    "                    (x, y), 1, 1, linewidth=1, edgecolor=\"black\", facecolor=\"white\"\n",
    "                )\n",
    "                self.ax.add_patch(rect)\n",
    "        self.ax.set_xlim(0, width)\n",
    "        self.ax.set_ylim(height, 0)\n",
    "        self.ax.set_aspect(\"equal\")\n",
    "\n",
    "        # Move x-axis labels to the top\n",
    "        self.ax.xaxis.set_label_position(\"top\")\n",
    "        self.ax.xaxis.tick_top()\n",
    "\n",
    "        # Draw target\n",
    "        # TODO: cater multiple goals\n",
    "        tx, ty = self.model.get_target_location()\n",
    "        target_patch = patches.Rectangle(\n",
    "            (tx, ty), 1, 1, linewidth=1, edgecolor=\"black\", facecolor=\"green\"\n",
    "        )\n",
    "        self.ax.add_patch(target_patch)\n",
    "\n",
    "    def draw_agent(self, info):\n",
    "        # Draw agent\n",
    "        for pos, has_item in info:\n",
    "            ax, ay = pos\n",
    "            agent_color = \"blue\" if not has_item else \"orange\"\n",
    "            agent_patch = patches.Circle((ax + 0.5, ay + 0.5), 0.3, color=agent_color)\n",
    "            self.ax.add_patch(agent_patch)\n",
    "\n",
    "    def draw_item(self, items):\n",
    "        for item in items:\n",
    "            ix, iy = item\n",
    "            item_patch = patches.Circle((ix + 0.5, iy + 0.5), 0.2, color=\"red\")\n",
    "            self.ax.add_patch(item_patch)\n",
    "\n",
    "    def draw_complete(self):\n",
    "        self.ax.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"Complete\",\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            transform=self.ax.transAxes,\n",
    "            fontsize=20,\n",
    "            color=\"red\",\n",
    "        )\n",
    "\n",
    "    # ----- ----- ----- ----- Render UI Element  ----- ----- ----- ----- #\n",
    "\n",
    "    def add_ui_elements(self):\n",
    "        self.init_buttons()\n",
    "        self.init_text()\n",
    "\n",
    "    def init_buttons(self):\n",
    "        # Add button for next step\n",
    "        self.next_step_btn = self.add_button(\n",
    "            [0.85, 0.01, 0.12, 0.075], \"Next Step\", self.on_next\n",
    "        )\n",
    "        # Add button for reset\n",
    "        self.reset_btn = self.add_button(\n",
    "            [0.85, 0.11, 0.12, 0.075], \"Reset\", self.on_reset\n",
    "        )\n",
    "        # Add button for animation on/off\n",
    "        self.toggle_anim_btn = self.add_button(\n",
    "            [0.85, 0.21, 0.12, 0.075], \"Anim\\nOn\", self.on_toggle_anim\n",
    "        )\n",
    "        # Add button for auto reset on/off\n",
    "        self.toggle_auto_reset_btn = self.add_button(\n",
    "            [0.85, 0.31, 0.12, 0.075], \"Auto Reset\\nOn\", self.on_auto_reset\n",
    "        )\n",
    "        # # Add button for training\n",
    "        # self.bg_train_btn = self.add_button(\n",
    "        #     [0.85, 0.41, 0.12, 0.075], \"Train 1000\", self.on_train(1000, blocking=False)\n",
    "        # )\n",
    "        # Add button for training\n",
    "        self.block_train_btn = self.add_button(\n",
    "            [0.85, 0.41, 0.12, 0.075],\n",
    "            \"Train 2500\",\n",
    "            self.on_train(2500, blocking=True),\n",
    "        )\n",
    "        # Add button for training grpah\n",
    "        self.show_graph_button = self.add_button(\n",
    "            [0.85, 0.51, 0.12, 0.075], \"Train Graph\", self.on_show_graph\n",
    "        )\n",
    "        # Add button for testing\n",
    "        self.test_button = self.add_button(\n",
    "            [0.85, 0.61, 0.12, 0.075], \"Test\", self.on_test(100, blocking=True)\n",
    "        )\n",
    "\n",
    "    def init_text(self):\n",
    "        # Add text box for cumulative reward\n",
    "        self.reward = self.add_text(\n",
    "            [0.01, 0.01, 0.2, 0.075], f\"Reward: {self.model.get_total_reward()}\"\n",
    "        )\n",
    "\n",
    "        # Add text box for max reward\n",
    "        self.max_reward = self.add_text(\n",
    "            [0.25, 0.01, 0.2, 0.075],\n",
    "            f\"Max Reward: {self.model.get_max_reward()}\",\n",
    "        )\n",
    "\n",
    "    def add_button(self, coordinates: Coordinates, text, on_click):\n",
    "        axis = plt.axes(coordinates)\n",
    "        # axis = self.ax\n",
    "        button = Button(axis, text)\n",
    "        button.on_clicked(on_click)\n",
    "\n",
    "        return button\n",
    "\n",
    "    def add_text(self, coordinates: Coordinates, text):\n",
    "        axis = plt.axes(coordinates)\n",
    "        # axis = self.ax\n",
    "        textbox = axis.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            text,\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            transform=axis.transAxes,\n",
    "            fontsize=12,\n",
    "        )\n",
    "        axis.axis(\"off\")\n",
    "        return textbox\n",
    "\n",
    "    # ----- ----- ----- ----- Event Handlers  ----- ----- ----- ----- #\n",
    "\n",
    "    def on_close(self, e):\n",
    "        pass\n",
    "\n",
    "    def on_show_graph(self, e):\n",
    "        fig2, ax2 = plt.subplots()\n",
    "        MLGraph(self.storage.ml_losses, fig2, ax2).show()\n",
    "\n",
    "    def on_test(self, episodes, blocking=False):\n",
    "        def non_blocking_test(e):\n",
    "            self.before_auto_train()\n",
    "            self.controller.test_in_background(episodes)\n",
    "            self.after_auto_train()\n",
    "\n",
    "        def blocking_test(e):\n",
    "            losses = self.controller.test(episodes)\n",
    "            fig2, ax2 = plt.subplots()\n",
    "            MLGraph(losses, fig2, ax2).show()\n",
    "\n",
    "        return blocking_test if blocking else non_blocking_test\n",
    "\n",
    "    def on_train(self, episodes, blocking=False):\n",
    "        def blocking_train(e):\n",
    "            ml_losses = self.controller.train(episodes)\n",
    "            fig2, ax2 = plt.subplots()\n",
    "            MLGraph(ml_losses, fig2, ax2).show()\n",
    "\n",
    "        def non_blocking_train(e):\n",
    "            self.before_auto_train()\n",
    "            self.controller.train_in_background()\n",
    "            self.after_auto_train()\n",
    "\n",
    "        return blocking_train if blocking else non_blocking_train\n",
    "\n",
    "    def on_auto_reset(self, event):\n",
    "        auto_reset_is_on = self.controller.toggle_auto_reset()\n",
    "        if auto_reset_is_on:\n",
    "            self.toggle_auto_reset_btn.label.set_text(\"Auto Reset\\nOn\")\n",
    "        else:\n",
    "            self.toggle_auto_reset_btn.label.set_text(\"Auto Reset\\nOff\")\n",
    "        plt.show()\n",
    "\n",
    "    def on_toggle_anim(self, event):\n",
    "        if self.animating:\n",
    "            self.toggle_anim_btn.label.set_text(\"Anim\\nOff\")\n",
    "        else:\n",
    "            self.toggle_anim_btn.label.set_text(\"Anim\\nOn\")\n",
    "\n",
    "        self.animating = not self.animating\n",
    "        plt.show()\n",
    "\n",
    "    def on_reset(self, event):\n",
    "        self.model.reset()\n",
    "        self.draw(self.get_info())\n",
    "\n",
    "    def on_next(self, e):\n",
    "        self.controller.next()\n",
    "        self.draw(self.get_info())\n",
    "\n",
    "    # ----- ----- ----- ----- Helper Functions  ----- ----- ----- ----- #\n",
    "\n",
    "    def before_auto_train(self):\n",
    "        self.animating = False\n",
    "        self.controller.reset()\n",
    "\n",
    "        self.toggle_anim_btn.label.set_text(\"Anim\\nOff\")\n",
    "        self.draw(self.get_info())\n",
    "\n",
    "    def after_auto_train(self):\n",
    "        self.animating = True\n",
    "        self.controller.reset()\n",
    "\n",
    "        self.toggle_anim_btn.label.set_text(\"Anim\\nOn\")\n",
    "        self.draw(self.get_info())\n",
    "\n",
    "    # ----- ----- ----- ----- Plot Metrics  ----- ----- ----- ----- #\n",
    "    def plot_training(results):\n",
    "        iterations, losses, total_rewards = results\n",
    "        # Create a figure with 1 row and 2 columns of subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "        # Plotting the loss in the first subplot\n",
    "        ax1.plot(iterations, losses, marker=\"o\", label=\"Loss\")\n",
    "        ax1.set_title(\"Iteration vs Loss\")\n",
    "        ax1.set_xlabel(\"Iteration Number\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "        # Plotting the total rewards in the second subplot\n",
    "        ax2.plot(\n",
    "            iterations, total_rewards, label=\"Total Reward\", color=\"orange\", marker=\"o\"\n",
    "        )\n",
    "        ax2.set_title(\"Epsilon decay across iteration\")\n",
    "        ax2.set_xlabel(\"Iteration Number\")\n",
    "        ax2.set_ylabel(\"Epsilon\")\n",
    "\n",
    "        # Display the plots\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Tuple\n",
    "\n",
    "class IVisual(ABC):\n",
    "    @abstractmethod\n",
    "    def get_agent_info(self) -> List[Tuple[Tuple[int, int], bool]]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_untaken_items(self) -> List[Tuple[int, int]]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_total_reward(self) -> int:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_max_reward(self) -> int:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_size(self) -> Tuple[int, int]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_target_location(self) -> List[Tuple[int, int]]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def has_ended(self) -> bool:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Model(IVisual):\n",
    "    def __init__(self):\n",
    "        # Agents\n",
    "        self.agents = []\n",
    "\n",
    "        # Grid\n",
    "        self.grid: \"Grid\" = None\n",
    "\n",
    "    def set_grid(self, grid: \"Grid\"):\n",
    "        self.grid = grid\n",
    "        return self\n",
    "\n",
    "    def add_agent(self, agent: \"Agent\"):\n",
    "        self.agents.append(agent)\n",
    "        return self\n",
    "\n",
    "    # ---- Public Getter Functions (For Visualisation) ----- #\n",
    "\n",
    "    def get_agent_info(self) -> List[Tuple[Tuple[int, int], bool]]:\n",
    "        \"\"\"\n",
    "        Output: List of\n",
    "                - Tuple of:\n",
    "                    - coordinate: (int, int)\n",
    "                    - has_item: bool\n",
    "        \"\"\"\n",
    "        has_items = map(lambda agent: agent.has_item(), self.agents)\n",
    "        return list(zip(self.grid.get_state().get_agent_positions(), has_items))\n",
    "\n",
    "    def get_untaken_items(self):\n",
    "        return self.grid.get_state().get_untaken_item_pos()\n",
    "\n",
    "    def get_total_reward(self):\n",
    "        return sum(map(lambda a: a.get_total_reward(), self.agents))\n",
    "\n",
    "    def get_max_reward(self):\n",
    "        return self.grid.get_max_reward()\n",
    "\n",
    "    def get_size(self):\n",
    "        return self.grid.get_size()\n",
    "\n",
    "    def get_target_location(self):\n",
    "        return self.grid.get_state().get_goal_positions()\n",
    "\n",
    "    def has_ended(self):\n",
    "        return self.grid.get_state().is_terminal()\n",
    "\n",
    "    # ---- Public Control Functions ----- #\n",
    "    def reset(self):\n",
    "        self.grid.reset()\n",
    "        for agent in self.agents:\n",
    "            agent.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Single Agent Object-Pickup Problem**\n",
    "\n",
    "## **Problem Description**\n",
    "\n",
    "This task involves an agent navigating a 4x4 grid world to pick up an item located at a random position `A` and delivering it to a random destination `B`. The agent must learn to complete this task as efficiently as possible, regardless of its starting position.\n",
    "\n",
    "## **Methodology**\n",
    "\n",
    "The agent uses Deep Q-learning to learn an optimal policy. The parameters for the agent are set as suggested by the assignment brief:\n",
    "\n",
    "|    **Parameter**    | **Value** |\n",
    "|:---------------|-----------:|\n",
    "| learning rate | 0.997 |\n",
    "| epsilon (explore ratio) | 1.0 |\n",
    "| epsilon decay | 0.997 |\n",
    "| minimum epsilon | 0.1 |\n",
    "| replay buffer size | 1000 |\n",
    "| batch size | 200 |\n",
    "| network copy frequency | 500 |\n",
    "\n",
    "The agent chooses an action with `ε`-greedy policy then stores the experience in replay memory D. When there is enough experience in D, randomly extract a minibatch of experiences and calculate the target value by\n",
    "$$\n",
    "\\begin{cases}\n",
    "    y_i = r_i & \\text{if done} \\\\\n",
    "    y_i = r_i + \\gamma \\max_{a'} \\^{Q}(s'_i, a'_i) & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then do one step of learning.\n",
    "\n",
    "To facilitate convergence, three changes were made besides using the given API and implementing the learning algorithm.\n",
    "\n",
    "1. **torch.tensors** are used throughout and GPU/MPS acceleration used whenever available\n",
    "2. constant 0.1 is added as noise to states to prevent dead neurons\n",
    "3. maximum of 50 steps can be spent in one game before resetting \n",
    "\n",
    "### **State Space**\n",
    "\n",
    "The state space is defined by the agent’s position on the grid, the location of the item (A), and the location of goal (B). Since our grid is 4 by 4, and we need to use one-hot encoding for machine learning, this translates into 48 bits, 16 for each position / location.\n",
    "\n",
    "If the item is picked up by agent, its corresponding bit will be 0, indicating that there is no item on the grid anymore.\n",
    "\n",
    "### **Action Space**\n",
    "\n",
    "The agent can perform one of four actions at any given time:\n",
    "\n",
    "- **Move North**\n",
    "- **Move South**\n",
    "- **Move West**\n",
    "- **Move East**\n",
    "\n",
    "These actions move the agent one step in the corresponding direction unless the movement would result in the agent hitting a wall, in which case the agent remains in the same position.\n",
    "\n",
    "### **Reward Structure**\n",
    "\n",
    "The reward structure is designed to guide the agent toward efficiently solving the task:\n",
    "\n",
    "|    **Event**    | **Reward** |\n",
    "|:---------------|-----------:|\n",
    "| Picking up the item at `A` | +50 |\n",
    "| Delivering the item to `B` | +50 |\n",
    "| Moving to an empty grid | -1 |\n",
    "| Hitting a wall | -10 |\n",
    "\n",
    "This reward system incentivizes the agent to quickly locate and pick up the item and then deliver it to the goal while penalizing unnecessary movements and collisions with walls.\n",
    "\n",
    "If the agent reaches `B` without the item, the reward would be -1 and the game would not terminate.\n",
    "\n",
    "## **Performance Metrics**\n",
    "\n",
    "### **ML Loss**\n",
    "To show the progress of learning wtih Deep Q-Learning, there is a graph showing MSE loss over each step / learn. This is the returned value from given skeleton divided by 200 to show the loss per experience.\n",
    "\n",
    "### **Loss**\n",
    "\n",
    "The loss from assignment 1 is also kept.\n",
    "\n",
    "To evaluate the agent’s learning progress, we track the loss between episodes.\n",
    "\n",
    "The loss for each iteration is calculated as the difference between the maximum possible reward and the actual reward obtained by the agent in that iteration. The minimum value of loss is thus zero.\n",
    "\n",
    "This loss is then plotted against the number of iterations to visualize the agent’s learning progress.\n",
    "\n",
    "\n",
    "#### **- Maximum Reward Calculation**\n",
    "\n",
    "For each episode, the maximum possible reward is calculated by determining the optimal route:\n",
    "\n",
    "1. **Agent → Item**\n",
    "- The Manhattan distance between the agent’s starting position and the item `A`.\n",
    "2. **Item → Goal**\n",
    "- The Manhattan distance between the item `A` and the goal `B`.\n",
    "\n",
    "The maximum possible reward is computed by subtracting these distances from 102, which is the sum of reward of:\n",
    "\n",
    "1. **Picking up item**\n",
    "1. **Reaching goal**\n",
    "1. **Compensation for overcalculation**\n",
    "- Reaching either reward tiles did not penalise at all, which was assumed in Manhattan distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Visualisation</u>\n",
    "\n",
    "We initialize a new game and train the agent for 500 times\n",
    "\n",
    "**<font color='red'>Note</font>: Most buttons in the following illustration causes undesired effect in \n",
    "Jupyter Notebook, except RESET**\n",
    "\n",
    "**<font color='red'>Note</font>: Restart kernel in between re-runs is suggested**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "width, height = 4, 4\n",
    "max_itr = 1000\n",
    "\n",
    "modell = Model()\n",
    "controller = Controller()\n",
    "\n",
    "storage = Storage(max_itr)\n",
    "trainer = Trainer(max_itr)\n",
    "\n",
    "agent = Agent(\n",
    "    0,\n",
    "    State.get_possible_states(width, height),\n",
    "    State.get_possible_actions(),\n",
    ")\n",
    "grid = Grid(width, height)\n",
    "grid.add_agent(agent)\n",
    "\n",
    "trainer.bind(modell, storage, grid, [agent])\n",
    "\n",
    "modell.set_grid(grid).add_agent(agent).reset()\n",
    "controller.bind(modell).add_helper(storage, trainer)\n",
    "\n",
    "trainer.train(2500)\n",
    "fig2, ax2 = plt.subplots()\n",
    "MLGraph(storage.ml_losses, fig2, ax2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots()\n",
    "vis = Visualization(fig1, ax1)\n",
    "vis.bind(modell, controller, storage).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
