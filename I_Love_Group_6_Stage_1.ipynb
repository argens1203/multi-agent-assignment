{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, idx, all_states, actions):\n",
    "        # Agent property (for illustration purposes)\n",
    "        self.is_having_item = False\n",
    "\n",
    "        self.actions = actions  # TODO: encode different action for different state. How to initialize Q-Table\n",
    "        self.idx = idx\n",
    "\n",
    "        # Initialize Q Table for all state-action to be 0\n",
    "        self.Q = np.zeros((all_states, len(actions)))\n",
    "        # for state in all_states:\n",
    "        # self.Q[state] = [0 for i in actions]\n",
    "\n",
    "        # Initialize Learning param\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = -1\n",
    "        self.gamma = 0.8\n",
    "        self.alpha = 0.1\n",
    "\n",
    "    # ----- Core Functions ----- #\n",
    "    def choose_action(self, state, explore=True):\n",
    "        if explore and np.random.rand() < self.epsilon:\n",
    "            return random.choice(self.actions)\n",
    "        else:\n",
    "            # Extract immutable state information\n",
    "            state_i = self.massage(state)\n",
    "            return self.actions[np.argmax(self.Q[state_i])]\n",
    "\n",
    "    def update_learn(self, state, action, reward, next_state, is_terminal, learn=True):\n",
    "        self.update(next_state)\n",
    "\n",
    "        # Extract immutable state information\n",
    "        state_i = self.massage(state)\n",
    "        nxt_state_i = self.massage(next_state)\n",
    "\n",
    "        if not learn:\n",
    "            return\n",
    "\n",
    "        # All states (including terminal states) have initial Q-values of 0 and thus there is no need for branching for handling terminal next state\n",
    "        self.Q[state_i][self.actions.index(action)] += self.alpha * (\n",
    "            reward\n",
    "            + self.gamma * np.max(self.Q[nxt_state_i])\n",
    "            - self.Q[state_i][self.actions.index(action)]\n",
    "        )\n",
    "\n",
    "        # Epsilon decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    # ----- Public Functions ----- #\n",
    "    def has_item(self):\n",
    "        return self.is_having_item\n",
    "\n",
    "    def update(self, state):\n",
    "        self.is_having_item = state.has_item()\n",
    "\n",
    "    def reset(self):\n",
    "        self.is_having_item = False\n",
    "\n",
    "    # ----- Private Functions ----- #\n",
    "    # Extract immutable information from State object\n",
    "    def massage(self, state):\n",
    "        return state.extract_state(self.idx)\n",
    "\n",
    "    def get_q_table(self):\n",
    "        return self.Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Empty:\n",
    "    def __init__(self, pos):\n",
    "        x, y = pos\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def interact(self, other: Agent):\n",
    "        return -1, (self.x, self.y)\n",
    "\n",
    "    def __copy__(self):\n",
    "        return Empty((self.x, self.y))\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        return self.__copy__()\n",
    "\n",
    "\n",
    "class Goal(Empty):\n",
    "    def __init__(self, pos):\n",
    "        x, y = pos\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.reached = False\n",
    "\n",
    "    def interact(self, other: Agent):\n",
    "        if other.has_item() and not self.reached:\n",
    "            self.reached = True\n",
    "            return 50, (self.x, self.y)\n",
    "        else:\n",
    "            return -1, (self.x, self.y)\n",
    "\n",
    "    def has_reached(self):\n",
    "        return self.reached\n",
    "\n",
    "    def __copy__(self):\n",
    "        copy = Goal((self.x, self.y))\n",
    "        copy.reached = self.reached\n",
    "        return copy\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        return self.__copy__()\n",
    "\n",
    "\n",
    "class Item(Empty):\n",
    "    def __init__(self, pos):\n",
    "        self.taken = False\n",
    "        x, y = pos\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def interact(self, other: Agent):\n",
    "        if not self.taken and not other.has_item():\n",
    "            self.taken = True\n",
    "            return 50, (self.x, self.y)\n",
    "\n",
    "        return -1, (self.x, self.y)\n",
    "\n",
    "    def get_pos(self):\n",
    "        return self.x, self.y\n",
    "\n",
    "    def __copy__(self):\n",
    "        copy = Item((self.x, self.y))\n",
    "        copy.taken = self.taken\n",
    "        return copy\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        return self.__copy__()\n",
    "\n",
    "\n",
    "class Wall(Empty):\n",
    "    def __init__(self, pos, dimensions):\n",
    "        x, y = pos\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "        width, height = dimensions\n",
    "        self.new_x = min(width - 1, max(0, x))\n",
    "        self.new_y = min(height - 1, max(0, y))\n",
    "\n",
    "    def interact(self, other: Agent):\n",
    "        return -10, (self.new_x, self.new_y)\n",
    "\n",
    "    def __copy__(self):\n",
    "        return Wall((self.x, self.y))\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        return self.__copy__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Array\n",
    "\n",
    "\n",
    "class Controller(object):\n",
    "    # Iterate by number of games\n",
    "    def __init__(self, game, max_itr):\n",
    "        self.game = game\n",
    "        self.timeout = 0.5\n",
    "        self.auto_reset = True\n",
    "        self.itr = 0\n",
    "        self.max_itr = max_itr\n",
    "\n",
    "        self.iterations = Array(\"i\", range(max_itr))\n",
    "        self.losses = Array(\"i\", max_itr)\n",
    "        self.epsilon = Array(\"f\", max_itr)\n",
    "\n",
    "        self.test_loss = Array(\"f\", max_itr)\n",
    "\n",
    "    def get_info(self):\n",
    "        info = self.game.get_agent_info()\n",
    "        items = self.game.get_untaken_items()\n",
    "        tot_reward = self.game.get_total_reward()\n",
    "        max_reward = self.game.get_max_reward()\n",
    "        return info, items, tot_reward, max_reward\n",
    "\n",
    "    def set_timeout(self, timeout):\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def toggle_auto_reset(self):\n",
    "        self.auto_reset = not self.auto_reset\n",
    "        return self.auto_reset\n",
    "\n",
    "    def next(self):\n",
    "        if self.game.has_ended() and self.auto_reset:\n",
    "            self.game.reset()\n",
    "        self.game.step(learn=False)\n",
    "        return self.get_info()\n",
    "\n",
    "    def train(self, itr=1):\n",
    "        self.game.reset()\n",
    "        for _ in range(itr):\n",
    "            (\n",
    "                loss,\n",
    "                reward,\n",
    "                epsilon,\n",
    "            ) = self.game.train_one_game()\n",
    "            if self.itr >= self.max_itr:\n",
    "                self.itr = 0\n",
    "            self.losses[self.itr] = loss\n",
    "            self.epsilon[self.itr] = epsilon\n",
    "            self.itr += 1\n",
    "\n",
    "    def test(self, itr=1):\n",
    "        self.game.reset()\n",
    "        for i in range(self.max_itr):\n",
    "            self.test_loss[i] = 0\n",
    "        for _ in range(itr):\n",
    "            (\n",
    "                loss,\n",
    "                reward,\n",
    "                epsilon,\n",
    "            ) = self.game.train_one_game(learn=False)\n",
    "            if self.itr >= self.max_itr:\n",
    "                self.itr = 0\n",
    "            self.test_loss[self.itr] = loss\n",
    "            self.itr += 1\n",
    "\n",
    "    def get_metrics(self):\n",
    "        return self.iterations, self.losses, self.epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from copy import deepcopy\n",
    "\n",
    "class Action:\n",
    "    NORTH = \"N\"\n",
    "    WEST = \"W\"\n",
    "    EAST = \"E\"\n",
    "    SOUTH = \"S\"\n",
    "\n",
    "\n",
    "class State:\n",
    "    def __init__(self, agent_positions, lookup):\n",
    "        self.agent_positions = agent_positions\n",
    "        self.lookup = deepcopy(lookup)\n",
    "\n",
    "    def get_possible_actions():\n",
    "        # Generate possible actions\n",
    "        return [Action.NORTH, Action.SOUTH, Action.EAST, Action.WEST]\n",
    "\n",
    "    def get_possible_states(width, height):\n",
    "        # Generate all possible states\n",
    "        return 5**5\n",
    "        positions = [(x, y) for x in range(width) for y in range(height)]\n",
    "        has_items = [True, False]\n",
    "        return itertools.product(positions, positions, has_items)\n",
    "\n",
    "    # ----- Private Functions ----- #\n",
    "    def get_goal(self):\n",
    "        return next((x for x in self.lookup if isinstance(x, Goal)), [None])\n",
    "\n",
    "    def get_items(self):\n",
    "        return [x for x in self.lookup if isinstance(x, Item)]\n",
    "\n",
    "    def get_item_positions(self):\n",
    "        return [item.get_pos() for item in self.get_items()]\n",
    "\n",
    "    def has_item(self):\n",
    "        item = next((x for x in self.lookup if isinstance(x, Item)), [None])\n",
    "        return item.taken\n",
    "\n",
    "    def extract_state(self, idx):\n",
    "        x, y = self.agent_positions[idx]\n",
    "        x2, y2 = self.get_item_positions()[0]\n",
    "        # TODO: remove hardcoded item_pos indices\n",
    "        # return agent_pos, item_pos[0], self.has_item()\n",
    "        return (\n",
    "            x * (5**4)\n",
    "            + y * (5**3)\n",
    "            + x2 * (5**2)\n",
    "            + y2 * (5)\n",
    "            + (1 if self.has_item() else 0)\n",
    "        )\n",
    "\n",
    "    # ----- Information Extraction ----- #\n",
    "    def get_agent_positions(self):\n",
    "        return self.agent_positions\n",
    "\n",
    "    def get_goal_positions(self):\n",
    "        goal = self.get_goal()\n",
    "        return goal.x, goal.y\n",
    "\n",
    "    def get_item_positions(self):\n",
    "        return [item.get_pos() for item in self.get_items()]\n",
    "\n",
    "    def is_terminal(self):\n",
    "        goal = self.get_goal()\n",
    "        return goal.has_reached()\n",
    "\n",
    "    def get_untaken_item_pos(self):\n",
    "        untaken_items = filter(lambda i: not i.taken, self.get_items())\n",
    "        return [i.get_pos() for i in untaken_items]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "class Game:\n",
    "    def __init__(self):\n",
    "        # Parameters\n",
    "        self.width = 5\n",
    "        self.height = 5\n",
    "        # Metrics\n",
    "        self.total_reward = 0\n",
    "\n",
    "        # Agents\n",
    "        self.agent = [\n",
    "            Agent(\n",
    "                idx,\n",
    "                State.get_possible_states(self.width, self.height),\n",
    "                State.get_possible_actions(),\n",
    "            )\n",
    "            for idx in range(1)\n",
    "        ]\n",
    "\n",
    "        # Grid\n",
    "        self.grid = Grid(self.width, self.height)\n",
    "        self.grid.add_agents(self.agent)\n",
    "        self.reset()\n",
    "\n",
    "    def train_one_game(self, learn=True):\n",
    "        self.reset()\n",
    "        self.total_reward = 0\n",
    "        max_reward = GridUtil.calculate_max_reward(self.grid)\n",
    "\n",
    "        max_step_count = 10000 if learn else 100\n",
    "        step_count = 0\n",
    "        while not self.grid.get_state().is_terminal() and step_count < max_step_count:\n",
    "            self.step(learn)\n",
    "            step_count += 1\n",
    "\n",
    "        loss = max_reward - self.total_reward\n",
    "        return loss, self.total_reward, self.agent[0].epsilon\n",
    "\n",
    "    # ---- Public Getter Functions (For Visualisation) ----- #\n",
    "\n",
    "    def get_agent_info(self) -> List[Tuple[Tuple[int, int], bool]]:\n",
    "        \"\"\"\n",
    "        Output: List of\n",
    "                - Tuple of:\n",
    "                    - coordinate: (int, int)\n",
    "                    - has_item: bool\n",
    "        \"\"\"\n",
    "        has_items = map(lambda agent: agent.has_item(), self.agent)\n",
    "        return list(zip(self.grid.get_state().get_agent_positions(), has_items))\n",
    "\n",
    "    def get_untaken_items(self):\n",
    "        return self.grid.get_state().get_untaken_item_pos()\n",
    "\n",
    "    def get_max_reward(self):\n",
    "        return self.max_reward\n",
    "\n",
    "    def get_size(self):\n",
    "        return self.width, self.height\n",
    "\n",
    "    def get_target_location(self):\n",
    "        return self.grid.get_state().get_goal_positions()\n",
    "\n",
    "    def has_ended(self):\n",
    "        return self.grid.get_state().is_terminal()\n",
    "\n",
    "    def get_total_reward(self):\n",
    "        return self.total_reward\n",
    "\n",
    "    # ---- Public Control Functions ----- #\n",
    "    def reset(self):\n",
    "        self.total_reward = 0\n",
    "        self.grid.reset()\n",
    "        for agent in self.agent:\n",
    "            agent.reset()\n",
    "        self.max_reward = GridUtil.calculate_max_reward(self.grid)\n",
    "\n",
    "    def step(self, learn=True):\n",
    "        if self.grid.get_state().is_terminal():\n",
    "            return\n",
    "        state = self.grid.get_state()\n",
    "\n",
    "        actions = [agent.choose_action(state, explore=learn) for agent in self.agent]\n",
    "        results = self.grid.move(actions)\n",
    "\n",
    "        for action, (reward, next_state, terminal), agent in zip(\n",
    "            actions, results, self.agent\n",
    "        ):\n",
    "            self.total_reward += reward\n",
    "            if learn:\n",
    "                agent.update_learn(state, action, reward, next_state, terminal)\n",
    "            else:\n",
    "                agent.update(next_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self, controller, fig, axs):\n",
    "        self.controller = controller\n",
    "        self.fig = fig\n",
    "        self.ax1, self.ax2 = axs\n",
    "\n",
    "        self.controller = controller\n",
    "        self.ani = animation.FuncAnimation(\n",
    "            self.fig, self.draw, frames=self.frames, interval=100, save_count=100\n",
    "        )\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def frames(self):\n",
    "        while True:\n",
    "            yield None\n",
    "\n",
    "    def draw(self, args):\n",
    "        self.plot_losses(\n",
    "            self.ax1,\n",
    "            self.controller.iterations,\n",
    "            self.controller.losses,\n",
    "        )\n",
    "        self.plot_epsilon(\n",
    "            self.ax2,\n",
    "            self.controller.iterations,\n",
    "            self.controller.epsilon,\n",
    "        )\n",
    "\n",
    "    def plot_losses(self, ax, iterations, loss):\n",
    "        # Plotting the loss in the first subplot\n",
    "        ax.plot(iterations, loss, color=\"blue\", label=\"Loss\")\n",
    "        ax.set_title(\"Loss\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "\n",
    "    def plot_epsilon(self, ax, iterations, epsilon):\n",
    "        # Plotting the loss in the first subplot\n",
    "        ax.plot(iterations, epsilon, color=\"blue\", label=\"Loss\")\n",
    "        ax.set_title(\"Epsilon\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.set_ylabel(\"Epsilon\")\n",
    "\n",
    "\n",
    "class TestGraph:\n",
    "    def __init__(self, controller, fig, ax):\n",
    "        self.controller = controller\n",
    "        self.fig = fig\n",
    "        self.ax = ax\n",
    "\n",
    "        self.controller = controller\n",
    "        self.ani = animation.FuncAnimation(\n",
    "            self.fig, self.draw, frames=self.frames, interval=100, save_count=100\n",
    "        )\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def frames(self):\n",
    "        while True:\n",
    "            yield None\n",
    "\n",
    "    def draw(self, args):\n",
    "        self.plot_losses(\n",
    "            self.ax,\n",
    "            self.controller.iterations,\n",
    "            self.controller.test_loss,\n",
    "        )\n",
    "\n",
    "    def plot_losses(self, ax, iterations, loss):\n",
    "        # Plotting the loss in the first subplot\n",
    "        ax.plot(iterations, loss, color=\"blue\", label=\"Loss\")\n",
    "        ax.set_title(\"Loss\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.set_ylabel(\"Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class GridFactory:\n",
    "    # Getting a random location in a grid, excluding certain locations\n",
    "    def get_random_pos(width, height, exclude=[]):\n",
    "        while True:\n",
    "            position = (\n",
    "                random.randint(0, width - 1),\n",
    "                random.randint(0, height - 1),\n",
    "            )\n",
    "            if position not in exclude:\n",
    "                return position\n",
    "\n",
    "\n",
    "class Grid:\n",
    "    def __init__(self, width=5, height=5):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "        self.state = {}  # TODO: multiple entities in one cell\n",
    "        self.lookup = set()  # Interactive tiles\n",
    "        self.agents = []\n",
    "        self.agent_positions = []\n",
    "\n",
    "        self.init_environment()\n",
    "\n",
    "    # ----- Init Functions ----- #\n",
    "    def init_environment(self):\n",
    "        for x in range(-1, self.width + 1):\n",
    "            for y in range(-1, self.height + 1):\n",
    "                if x < 0 or x >= self.width:\n",
    "                    self.state[(x, y)] = Wall((x, y), (self.width, self.height))\n",
    "                elif y < 0 or y >= self.height:\n",
    "                    self.state[(x, y)] = Wall((x, y), (self.width, self.height))\n",
    "                else:\n",
    "                    self.state[(x, y)] = Empty((x, y))\n",
    "\n",
    "    # ----- Core Functions ----- #\n",
    "    def move(self, actions):  # List of actions, in the same order as self.agents\n",
    "        # Update agent to temporary location according to move\n",
    "        temp_positions = [\n",
    "            self.process_action(action, agent_pos)\n",
    "            for action, agent_pos in zip(actions, self.agent_positions)\n",
    "        ]\n",
    "\n",
    "        # Retreive reward and new location according to Entity.interaction\n",
    "        reward_new_positions = [\n",
    "            self.state[(x, y)].interact(agent)\n",
    "            for agent, (x, y) in zip(self.agents, temp_positions)\n",
    "        ]\n",
    "        rewards, new_positions = zip(*reward_new_positions)\n",
    "\n",
    "        # Update new positions\n",
    "        self.agent_positions = new_positions\n",
    "\n",
    "        # Return move results, in the same order as self.agents\n",
    "        return [\n",
    "            (reward, self.get_state(), self.get_state().is_terminal())\n",
    "            for reward in rewards\n",
    "        ]\n",
    "\n",
    "    # ----- Private Functions ----- #\n",
    "    def process_action(self, action, agent_position):\n",
    "        # Move according to action\n",
    "        x, y = agent_position\n",
    "        dx, dy = self.interpret_action(action)\n",
    "        return x + dx, y + dy\n",
    "\n",
    "    def interpret_action(self, action):\n",
    "        if action == Action.NORTH:\n",
    "            return 0, -1\n",
    "        if action == Action.SOUTH:\n",
    "            return 0, 1\n",
    "        if action == Action.EAST:\n",
    "            return 1, 0\n",
    "        if action == Action.WEST:\n",
    "            return -1, 0\n",
    "\n",
    "    def set_interactive_tiles(self):\n",
    "        self.lookup.clear()\n",
    "        used_pos = []\n",
    "\n",
    "        # TODO: extract repeated code\n",
    "\n",
    "        # Assign goal to set position\n",
    "        goal_pos = (self.width - 1, self.height - 1)\n",
    "        goal = Goal(goal_pos)\n",
    "        self.state[goal_pos] = goal\n",
    "        self.lookup.add(goal)\n",
    "        used_pos.append(goal_pos)\n",
    "\n",
    "        # Assign items to a random position in the remaining tiles\n",
    "        item_pos = GridFactory.get_random_pos(self.width, self.height, used_pos)\n",
    "        item = Item(item_pos)\n",
    "        self.state[item_pos] = item\n",
    "        self.lookup.add(item)\n",
    "        used_pos.append(item_pos)\n",
    "\n",
    "        # Assign agents to random positions\n",
    "        self.agent_positions = []\n",
    "        for _ in self.agents:\n",
    "            agent_pos = GridFactory.get_random_pos(self.width, self.height, used_pos)\n",
    "            used_pos.append(agent_pos)\n",
    "            self.agent_positions.append(agent_pos)\n",
    "\n",
    "        # Future proofing: update agents in case they spwaned on an item\n",
    "        for agent in self.agents:\n",
    "            agent.update(State(self.agent_positions, self.lookup))\n",
    "\n",
    "    # ----- Public Functions ----- #\n",
    "    def reset(self):\n",
    "        self.init_environment()\n",
    "        self.set_interactive_tiles()\n",
    "\n",
    "    def add_agents(self, agents):\n",
    "        self.agents = agents\n",
    "\n",
    "    def get_state(self):\n",
    "        return State(self.agent_positions, self.lookup)\n",
    "\n",
    "\n",
    "class GridUtil:\n",
    "    def calculate_max_reward(grid):\n",
    "        # TODO: can only work with one agent and one item ATM\n",
    "        x1, y1 = grid.get_state().get_agent_positions()[0]\n",
    "        x2, y2 = grid.get_state().get_item_positions()[0]\n",
    "        x3, y3 = grid.get_state().get_goal_positions()\n",
    "\n",
    "        # Manhanttan distance from agent to obj and obj to goal\n",
    "        dist_to_obj = abs(x1 - x2) + abs(y1 - y2)\n",
    "        dist_to_goal = abs(x2 - x3) + abs(y2 - y3)\n",
    "\n",
    "        # +100 for reward and +2 for 2 unneeded mark deduction when stepping on item and goal respectively\n",
    "        return (dist_to_obj + dist_to_goal) * -1 + 102\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# import plotly\n",
    "from matplotlib.widgets import Button, Slider\n",
    "import matplotlib.animation as animation\n",
    "from typing import Tuple, TypeAlias, TYPE_CHECKING\n",
    "import json\n",
    "\n",
    "Coordinates: TypeAlias = Tuple[float, float, float, float]\n",
    "\n",
    "# plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "import numpy as np\n",
    "from multiprocessing import Process, Queue, shared_memory, Pipe\n",
    "\n",
    "\n",
    "class Visualization:\n",
    "    def __init__(self, game: \"Game\", controller, fig, ax):\n",
    "        self.game = game\n",
    "        self.is_stopping = False\n",
    "        self.timer = None\n",
    "        self.game.reset()\n",
    "        self.speed = 1\n",
    "        self.fig = fig\n",
    "        self.ax = ax\n",
    "\n",
    "        self.add_ui_elements()\n",
    "        self.controller = controller\n",
    "        self.fig.canvas.mpl_connect(\"close_event\", self.on_close)\n",
    "        self.ani = animation.FuncAnimation(\n",
    "            self.fig, self.draw, frames=self.frames, interval=200, save_count=100\n",
    "        )\n",
    "\n",
    "        self.animating = True\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def frames(self):\n",
    "        while True:\n",
    "            yield self.controller.next()\n",
    "\n",
    "    def draw(self, args):\n",
    "        info, items, tot_reward, max_reward = args\n",
    "\n",
    "        self.ax.clear()\n",
    "        self.draw_grid()\n",
    "        self.draw_agent(info)\n",
    "        self.draw_item(items)\n",
    "\n",
    "        self.reward.set_text(f\"Reward: {tot_reward}\")\n",
    "        self.max_reward.set_text(f\"Max Reward: {max_reward}\")\n",
    "\n",
    "        # Check if the environment is terminal\n",
    "        if self.game.has_ended():\n",
    "            self.draw_complete()\n",
    "        if not self.animating:\n",
    "            self.fig.canvas.draw()\n",
    "\n",
    "    def draw_grid(self):\n",
    "        width, height = self.game.get_size()\n",
    "        for x in range(width):\n",
    "            for y in range(height):\n",
    "                rect = patches.Rectangle(\n",
    "                    (x, y), 1, 1, linewidth=1, edgecolor=\"black\", facecolor=\"white\"\n",
    "                )\n",
    "                self.ax.add_patch(rect)\n",
    "        self.ax.set_xlim(0, width)\n",
    "        self.ax.set_ylim(height, 0)\n",
    "        self.ax.set_aspect(\"equal\")\n",
    "\n",
    "        # Move x-axis labels to the top\n",
    "        self.ax.xaxis.set_label_position(\"top\")\n",
    "        self.ax.xaxis.tick_top()\n",
    "\n",
    "        # Draw target\n",
    "        tx, ty = self.game.get_target_location()\n",
    "        target_patch = patches.Rectangle(\n",
    "            (tx, ty), 1, 1, linewidth=1, edgecolor=\"black\", facecolor=\"green\"\n",
    "        )\n",
    "        self.ax.add_patch(target_patch)\n",
    "\n",
    "    def draw_agent(self, info):\n",
    "        # Draw agent\n",
    "        for pos, has_item in info:\n",
    "            ax, ay = pos\n",
    "            agent_color = \"blue\" if not has_item else \"orange\"\n",
    "            agent_patch = patches.Circle((ax + 0.5, ay + 0.5), 0.3, color=agent_color)\n",
    "            self.ax.add_patch(agent_patch)\n",
    "\n",
    "    def draw_item(self, items):\n",
    "        for item in items:\n",
    "            ix, iy = item\n",
    "            item_patch = patches.Circle((ix + 0.5, iy + 0.5), 0.2, color=\"red\")\n",
    "            self.ax.add_patch(item_patch)\n",
    "\n",
    "    def draw_complete(self):\n",
    "        self.ax.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"Complete\",\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            transform=self.ax.transAxes,\n",
    "            fontsize=20,\n",
    "            color=\"red\",\n",
    "        )\n",
    "\n",
    "    # ----- ----- ----- ----- Render UI Element  ----- ----- ----- ----- #\n",
    "\n",
    "    def add_ui_elements(self):\n",
    "        self.init_buttons()\n",
    "        self.init_text()\n",
    "\n",
    "    def init_buttons(self):\n",
    "        # Add button for next step\n",
    "        self.next_step_btn = self.add_button(\n",
    "            [0.85, 0.01, 0.12, 0.075], \"Next Step\", self.on_next\n",
    "        )\n",
    "        # Add button for reset\n",
    "        self.reset_btn = self.add_button(\n",
    "            [0.85, 0.11, 0.12, 0.075], \"Reset\", self.on_reset\n",
    "        )\n",
    "        # Add button for animation on/off\n",
    "        self.toggle_anim_btn = self.add_button(\n",
    "            [0.85, 0.21, 0.12, 0.075], \"Anim\\nOn\", self.on_toggle_anim\n",
    "        )\n",
    "        # Add button for auto reset on/off\n",
    "        self.toggle_auto_reset_btn = self.add_button(\n",
    "            [0.85, 0.31, 0.12, 0.075], \"Auto Reset\\nOn\", self.on_auto_reset\n",
    "        )\n",
    "        # Add button for training\n",
    "        self.train_1000_btn = self.add_button(\n",
    "            [0.85, 0.41, 0.12, 0.075], \"Train 1000\", self.on_train_1000\n",
    "        )\n",
    "        # Add button for training\n",
    "        self.train_15000_btn = self.add_button(\n",
    "            [0.85, 0.51, 0.12, 0.075], \"Train 15000\", self.on_train_15000\n",
    "        )\n",
    "        # Add button for training\n",
    "        self.test_button = self.add_button(\n",
    "            [0.85, 0.61, 0.12, 0.075], \"Test\", self.on_test\n",
    "        )\n",
    "\n",
    "    def init_text(self):\n",
    "        # Add text box for cumulative reward\n",
    "        self.reward = self.add_text(\n",
    "            [0.01, 0.01, 0.2, 0.075], f\"Reward: {self.game.total_reward}\"\n",
    "        )\n",
    "\n",
    "        # Add text box for max reward\n",
    "        self.max_reward = self.add_text(\n",
    "            [0.25, 0.01, 0.2, 0.075],\n",
    "            f\"Max Reward: {self.game.get_max_reward()}\",\n",
    "        )\n",
    "\n",
    "    def add_button(self, coordinates: Coordinates, text, on_click):\n",
    "        axis = plt.axes(coordinates)\n",
    "        # axis = self.ax\n",
    "        button = Button(axis, text)\n",
    "        button.on_clicked(on_click)\n",
    "\n",
    "        return button\n",
    "\n",
    "    def add_text(self, coordinates: Coordinates, text):\n",
    "        axis = plt.axes(coordinates)\n",
    "        # axis = self.ax\n",
    "        textbox = axis.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            text,\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            transform=axis.transAxes,\n",
    "            fontsize=12,\n",
    "        )\n",
    "        axis.axis(\"off\")\n",
    "        return textbox\n",
    "\n",
    "    # ----- ----- ----- ----- Render Main Board  ----- ----- ----- ----- #\n",
    "    def one_step(self):\n",
    "        self.draw(self.controller.next())\n",
    "\n",
    "    def stop_anim(self):\n",
    "        pass\n",
    "\n",
    "    def start_anim(self):\n",
    "        pass\n",
    "\n",
    "    # ----- ----- ----- ----- Event Handlers  ----- ----- ----- ----- #\n",
    "\n",
    "    def on_toggle_anim(self, event):\n",
    "        if self.animating:\n",
    "            self.ani.pause()\n",
    "            self.toggle_anim_btn.label.set_text(\"Anim\\nOff\")\n",
    "        else:\n",
    "            self.ani.resume()\n",
    "            self.toggle_anim_btn.label.set_text(\"Anim\\nOn\")\n",
    "\n",
    "        self.animating = not self.animating\n",
    "        plt.show()\n",
    "\n",
    "    def on_auto_reset(self, event):\n",
    "        auto_reset_is_on = self.controller.toggle_auto_reset()\n",
    "        if auto_reset_is_on:\n",
    "            self.toggle_auto_reset_btn.label.set_text(\"Auto Reset\\nOn\")\n",
    "        else:\n",
    "            self.toggle_auto_reset_btn.label.set_text(\"Auto Reset\\nOff\")\n",
    "        plt.show()\n",
    "\n",
    "    def on_reset(self, event):\n",
    "        self.game.reset()\n",
    "        self.draw(self.controller.get_info())\n",
    "\n",
    "    def on_next(self, e):\n",
    "        self.draw(self.controller.next())\n",
    "\n",
    "    def on_train_1000(self, e):\n",
    "        self.before_auto_train()\n",
    "\n",
    "        s = self.auto_train()\n",
    "\n",
    "        self.game.agent[0].Q = self.get_np_from_name(s)\n",
    "\n",
    "        self.after_auto_train()\n",
    "\n",
    "    def before_auto_train(self):\n",
    "        self.ani.pause()\n",
    "        self.animating = False\n",
    "        self.controller.game.reset()\n",
    "\n",
    "        self.toggle_anim_btn.label.set_text(\"Anim\\nOff\")\n",
    "        self.draw(self.controller.get_info())\n",
    "\n",
    "    def auto_train(self):\n",
    "        gp, tp, conn1 = get_process(self.game, self.controller)\n",
    "        gp.start()\n",
    "        tp.start()\n",
    "        gp.join()\n",
    "        tp.join()\n",
    "        return conn1.recv()\n",
    "\n",
    "    def after_auto_train(self):\n",
    "        self.ani.resume()\n",
    "        self.animating = True\n",
    "        self.controller.game.reset()\n",
    "\n",
    "        self.toggle_anim_btn.label.set_text(\"Anim\\nOn\")\n",
    "        self.draw(self.controller.get_info())\n",
    "\n",
    "    def get_np_from_name(self, name):\n",
    "        existing_shm = shared_memory.SharedMemory(name=name)\n",
    "        q = np.ndarray((5**5, 4), buffer=existing_shm.buf)\n",
    "        s = np.copy(q)\n",
    "        existing_shm.close()\n",
    "        existing_shm.unlink()\n",
    "        return s\n",
    "\n",
    "    def on_train_15000(self, e):\n",
    "        self.before_auto_train()\n",
    "        self.controller.train(15000)\n",
    "        self.after_auto_train()\n",
    "\n",
    "    def np_to_name(self, np):\n",
    "        pass\n",
    "\n",
    "    def on_close(self, e):\n",
    "        pass\n",
    "\n",
    "    def on_test(self, e):\n",
    "        self.before_auto_train()\n",
    "        gp, tp = get_test_process(self.controller)\n",
    "        gp.start()\n",
    "        tp.start()\n",
    "        gp.join()\n",
    "        tp.join()\n",
    "        self.after_auto_train()\n",
    "\n",
    "    # ----- ----- ----- ----- Plot Metrics  ----- ----- ----- ----- #\n",
    "    def plot_training(results):\n",
    "        iterations, losses, total_rewards = results\n",
    "        # Create a figure with 1 row and 2 columns of subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "        # Plotting the loss in the first subplot\n",
    "        ax1.plot(iterations, losses, marker=\"o\", label=\"Loss\")\n",
    "        ax1.set_title(\"Iteration vs Loss\")\n",
    "        ax1.set_xlabel(\"Iteration Number\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "        # Plotting the total rewards in the second subplot\n",
    "        ax2.plot(\n",
    "            iterations, total_rewards, label=\"Total Reward\", color=\"orange\", marker=\"o\"\n",
    "        )\n",
    "        ax2.set_title(\"Epsilon decay across iteration\")\n",
    "        ax2.set_xlabel(\"Iteration Number\")\n",
    "        ax2.set_ylabel(\"Epsilon\")\n",
    "\n",
    "        # Display the plots\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def draw_graphs(game, controller):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    graph = Graph(controller, fig, axs)\n",
    "\n",
    "\n",
    "def train(controller, connection, ep):\n",
    "    controller.train(ep)\n",
    "    q = controller.game.agent[0].get_q_table()\n",
    "\n",
    "    shm = shared_memory.SharedMemory(create=True, size=q.nbytes)\n",
    "    b = np.ndarray(q.shape, dtype=q.dtype, buffer=shm.buf)\n",
    "    b[:] = q[:]\n",
    "    connection.send(shm.name)\n",
    "    shm.close()\n",
    "\n",
    "\n",
    "def get_process(game, controller):\n",
    "    conn1, conn2 = Pipe()\n",
    "    graph_p = Process(\n",
    "        target=draw_graphs,\n",
    "        args=[\n",
    "            game,\n",
    "            controller,\n",
    "        ],\n",
    "    )\n",
    "    train_p = Process(target=train, args=[controller, conn2, 1000])\n",
    "    return graph_p, train_p, conn1\n",
    "\n",
    "\n",
    "def test(controller, ep):\n",
    "    controller.test(ep)\n",
    "\n",
    "\n",
    "def draw_test_graph(controller):\n",
    "    fig, axs = plt.subplots()\n",
    "    graph = TestGraph(controller, fig, axs)\n",
    "\n",
    "\n",
    "def get_test_process(controller):\n",
    "    graph_p = Process(\n",
    "        target=draw_test_graph,\n",
    "        args=[\n",
    "            controller,\n",
    "        ],\n",
    "    )\n",
    "    test_p = Process(target=test, args=[controller, 1000])\n",
    "    return graph_p, test_p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Single Agent Object-Pickup Problem**\n",
    "\n",
    "## **Problem Description**\n",
    "\n",
    "This task involves an agent navigating a 5x5 grid world to pick up an item located at a random position `A` and delivering it to a fixed destination `B`, located at the bottom-right corner of the grid. The agent must learn to complete this task as efficiently as possible, regardless of its starting position.\n",
    "\n",
    "## **Methodology**\n",
    "\n",
    "The agent uses Q-learning to learn an optimal policy. The algorithm updates the Q-values stored in a Q-table based on the agent's interactions with the environment, gradually improving its strategy over time.\n",
    "Q table is initialised at ...\n",
    "The learning rate Alpha is set at 0.1\n",
    "\n",
    "### **State Space**\n",
    "\n",
    "The state space is defined by the agent’s position on the grid, the location of the item (A), and whether the agent is carrying the item. This can be represented as a tuple `((agent_coor_x, agent_coor_y), has_item)` where `has_item` is a boolean variable indicating if the agent has picked up the item.\n",
    "\n",
    "### **Action Space**\n",
    "\n",
    "The agent can perform one of four actions at any given time:\n",
    "\n",
    "- **Move North**\n",
    "- **Move South**\n",
    "- **Move West**\n",
    "- **Move East**\n",
    "\n",
    "These actions move the agent one step in the corresponding direction unless the movement would result in the agent hitting a wall, in which case the agent remains in the same position.\n",
    "\n",
    "### **Reward Structure**\n",
    "\n",
    "The reward structure is designed to guide the agent toward efficiently solving the task:\n",
    "\n",
    "|    **Event**    | **Reward** |\n",
    "|:---------------|-----------:|\n",
    "| Picking up the item at `A` | +50 |\n",
    "| Delivering the item to `B` | +50 |\n",
    "| Moving to an empty grid | -1 |\n",
    "| Hitting a wall | -10 |\n",
    "\n",
    "This reward system incentivizes the agent to quickly locate and pick up the item and then deliver it to the goal while penalizing unnecessary movements and collisions with walls.\n",
    "\n",
    "## **Performance Metrics**\n",
    "\n",
    "### **Loss-against-Iteration Graph**\n",
    "\n",
    "To evaluate the agent’s learning progress, we track the loss against the number of iterations.\n",
    "\n",
    "#### **Maximum Reward Calculation**\n",
    "\n",
    "For each episode, the maximum possible reward is calculated by determining the optimal route:\n",
    "\n",
    "1. **Agent → Item**: Calculate the Manhattan distance between the agent’s starting position and the item's location.\n",
    "2. **Item → Goal**: Calculate the Manhattan distance between the item's location and the goal at `B`.\n",
    "\n",
    "The maximum possible reward is computed by subtracting the sum of these distances from 102 (which includes the reward for picking up the item, delivering it to the goal, and a 2-point compensation for the optimal path).\n",
    "\n",
    "#### **Loss Calculation**\n",
    "\n",
    "The loss for each iteration is calculated as the difference between the maximum possible reward and the actual reward obtained by the agent in that iteration. This loss is then plotted against the number of iterations to visualize the agent’s learning progress.\n",
    "\n",
    "### **Epsilon Decay Graph**\n",
    "\n",
    "The Epsilon decay graph illustrates how the exploration rate (`ε`) changes over time. Initially, the agent explores more (`high ε`), but as learning progresses, `ε` decays, leading the agent to exploit known information more often. This graph provides insight into the balance between exploration and exploitation throughout the training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Visualisation</u>\n",
    "\n",
    "- Press Reset\n",
    "\n",
    "We initialize a new game and train the agent for 500 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game()\n",
    "times = 500\n",
    "controller = Controller(game, times)\n",
    "controller.train(times)\n",
    "fig1, ax1 = plt.subplots()\n",
    "vis = Visualization(game, controller, fig1, ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training it for 500 times, we can see that the agent can sparingly complete the goal, but more often times it oscillates between 2 cells, or being stuck at the corner/near a wall, not completing the task. We can see that the performance metrics below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualization.plot_training(controller.get_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, The loss at 500th iteration hasn't converged to 0, there is still room for improvement for our agent.\n",
    "\n",
    "Now, we allow it to train 1000 more times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game2 = Game()\n",
    "times = 2000\n",
    "controller2 = Controller(game2, times)\n",
    "controller2.train(times)\n",
    "fig2, ax2 = plt.subplots()\n",
    "vis2 = Visualization(game2, controller2, fig2, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Visualization.plot_training(controller2.get_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics graph is much better than before but we still havent converged yet. Lets train the agent up to 3000 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game3 = Game()\n",
    "times = 4000\n",
    "controller3 = Controller(game3, times)\n",
    "controller3.train(times)\n",
    "fig3, ax3 = plt.subplots()\n",
    "vis3 = Visualization(game3, controller3, fig3, ax3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Visualization.plot_training(controller3.get_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If we observe the final few hundred iterations of the training, we can see that the loss is almost consistently 0. We can say that it has converged and the agent has fully learnt about the problem set.\n",
    "\n",
    "# Conclusion\n",
    "...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
